{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e90d5c",
   "metadata": {},
   "source": [
    "# Deploy Llama 2 on AWS inferentia2 with Amazon SageMaker\n",
    "\n",
    "In this end-to-end tutorial, you will learn how to deploy and speed up Llama 2 inference using AWS Inferentia2 and [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index) on Amazon SageMaker. [Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index) is the interface between the Hugging Face Transformers & Diffusers library and AWS Accelerators including AWS Trainium and AWS Inferentia2. \n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "1. Convert Llama 2 to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "2. Create a custom `inference.py` script for Llama 2\n",
    "3. Upload the neuron model and inference script to Amazon S3\n",
    "4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "5. Generate images using the deployed model\n",
    "\n",
    "## Quick intro: AWS Inferentia 2\n",
    "\n",
    "[AWS inferentia (Inf2)](https://aws.amazon.com/de/ec2/instance-types/inf2/) are purpose-built EC2 for deep learning (DL) inference workloads. Inferentia 2 is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls), which promises to deliver up to 4x higher throughput and up to 10x lower latency.\n",
    "\n",
    "| instance size | accelerators | Neuron Cores | accelerator memory | vCPU | CPU Memory | on-demand price ($/h) |\n",
    "| ------------- | ------------ | ------------ | ------------------ | ---- | ---------- | --------------------- |\n",
    "| inf2.xlarge   | 1            | 2            | 32                 | 4    | 16         | 0.76                  |\n",
    "| inf2.8xlarge  | 1            | 2            | 32                 | 32   | 128        | 1.97                  |\n",
    "| inf2.24xlarge | 6            | 12           | 192                | 96   | 384        | 6.49                  |\n",
    "| inf2.48xlarge | 12           | 24           | 384                | 192  | 768        | 12.98                 |\n",
    "\n",
    "Additionally, inferentia 2 will support the writing of custom operators in c++ and new datatypes, including `FP8` (cFP8).\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can findÂ [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)Â more about it.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3db68e5",
   "metadata": {},
   "source": [
    "## 1. Convert Llama 2 to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "\n",
    "We are going to use the [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index) to compile/convert our model to neuronx. Optimum Neuron provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks. \n",
    "\n",
    "As a first step, we need to install the `optimum-neuron` and other required packages.\n",
    "\n",
    "*Tip: If you are using Amazon SageMaker Notebook Instances or Studio you can go with the `conda_python3` conda kernel.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install \"optimum-neuron==0.0.13\" --upgrade\n",
    "%pip install \"sagemaker>=2.197.0\"  --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0ef431",
   "metadata": {},
   "source": [
    "After we have installed the `optimum-neuron` we can convert load and convert our model.\n",
    "\n",
    "We are going to use the [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) model. Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases.\n",
    "\n",
    "At the time of writing, the [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/dynamic-shapes.html?highlight=dynamic%20shapes#), which means that the we need to specify our image size in advanced for compiling and inference. \n",
    "\n",
    "In simpler terms, this means we need to define the input shapes for our prompt (sequence length), batch size, height and width of the image.\n",
    "\n",
    "We precompiled the model with the following parameters and pushed it to the Hugging Face Hub: \n",
    "* `sequence_length`: 2048\n",
    "* `batch_size`: 4\n",
    "* `neuron`: 2.15.0\n",
    "\n",
    "\n",
    "_Note: If you want to compile your own model or a different Llama 2 checkpoint you need to use ~120GB of memory and the compilation can take ~60 minutes. We used an `inf2.24xlarge` ec2 instance with the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2) to compile the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# compiled model id\n",
    "compiled_model_id = \"aws-neuron/Llama-2-7b-chat-hf-seqlen-2048-bs-2\"\n",
    "\n",
    "# save compiled model to local directory\n",
    "save_directory = \"llama_neuron\"\n",
    "# Downloads our compiled model from the HuggingFace Hub \n",
    "# using the revision as neuron version reference\n",
    "# and makes sure we exlcude the symlink files and \"hidden\" files, like .DS_Store, .gitignore, etc.\n",
    "snapshot_download(compiled_model_id, revision=\"2.15.0\", local_dir=save_directory, local_dir_use_symlinks=False, allow_patterns=[\"[!.]*.*\"])\n",
    "\n",
    "\n",
    "###############################################\n",
    "# COMMENT IN BELOW TO COMPILE DIFFERENT MODEL #\n",
    "###############################################\n",
    "#\n",
    "# from optimum.neuron import NeuronStableDiffusionXLPipeline\n",
    "# from transformers import AutoTokenizer\n",
    "# \n",
    "# # model id you want to compile\n",
    "# vanilla_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#\n",
    "# # configs for compiling model\n",
    "# compiler_args = {\"num_cores\": 2, \"auto_cast_type\": \"fp16\"}\n",
    "# input_shapes = {\n",
    "#   \"sequence_length\": 2048, # max length to generate\n",
    "#   \"batch_size\": 1 # batch size for the model\n",
    "#   }\n",
    "#\n",
    "# llm = NeuronModelForCausalLM.from_pretrained(vanilla_model_id, export=True, **input_shapes, **compiler_args)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# \n",
    "# # Save locally or upload to the HuggingFace Hub\n",
    "# save_directory = \"llama_neuron\"\n",
    "# llm.save_pretrained(save_directory)\n",
    "# tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9997e9db",
   "metadata": {},
   "source": [
    "## 2. Create a custom `inference.py` script for Stable Diffusion\n",
    "\n",
    "The [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) supports zero-code deployments on top of theÂ [pipelineÂ feature](https://huggingface.co/transformers/main_classes/pipelines.html)Â from ðŸ¤— Transformers. This allows users to deploy Hugging Face transformers without an inference script [[Example](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)]. \n",
    "\n",
    "Currently is this feature not supported with AWS Inferentia2, which means we need to provide an `inference.py` for running inference. But `optimum-neuron` has integrated support for the ðŸ¤— Diffusers pipeline feature. That way we can use the `optimum-neuron` to create a pipeline for our model.\n",
    "\n",
    "If you want to know more about the `inference.py`Â script check out this [example](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb). It explains amongst other things what the `model_fn` and `predict_fn` are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create code directory in our model directory\n",
    "!mkdir {save_directory}/code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce675df9",
   "metadata": {},
   "source": [
    "We are using the `NEURON_RT_NUM_CORES=2` to make sure that each HTTP worker uses 2 Neuron core for inference. In additon we are going to use [\"templates for chat models\"](https://huggingface.co/docs/transformers/main/en/chat_templating) and new feature of transformers, which allows us to provide OpenAI messages, which are then converted to the correct input format for the model.\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi there!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "For this to work we need `jinja2` installed. Lets create a `requirements.txt` file and install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44509f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llama_neuron/code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {save_directory}/code/requirements.txt\n",
    "\n",
    "jinja2>=\"3.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ff7c1",
   "metadata": {},
   "source": [
    "Now, we create our `inference.py` file using the `apply_chat_template` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llama_neuron/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {save_directory}/code/inference.py\n",
    "import os\n",
    "# To use two neuron core per worker\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"2\"\n",
    "import torch\n",
    "import torch_neuronx\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from optimum.neuron import pipeline\n",
    "import time\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load local converted model and tokenizer\n",
    "    pipe = pipeline(\"text-generation\", model_dir)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_fn(data, pipe):\n",
    "    # extract prompt from data\n",
    "    messages = data.pop(\"inputs\", data)\n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "    \n",
    "    # convert messages to input ids\n",
    "    inputs = pipe.tokenizer.apply_chat_template(messages, add_generation_prompt=True,tokenize=False)\n",
    "    # run generation\n",
    "    if parameters is not None:\n",
    "        outputs = pipe(inputs, **parameters)[0]\n",
    "    else:\n",
    "        outputs = pipe(inputs)[0]\n",
    "\n",
    "    # decode generation \n",
    "    return {\"generation\": outputs[\"generated_text\"][len(inputs):].strip()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## 3. Upload the neuron model and inference script to Amazon S3\n",
    "\n",
    "Before we can deploy our neuron model to Amazon SageMaker we need to upload it all our model artifacts to Amazon S3.\n",
    "\n",
    "_Note: Currently `inf2` instances are only available in the `us-east-2` & `us-east-1` region [[REF](https://aws.amazon.com/de/about-aws/whats-new/2023/05/sagemaker-ml-inf2-ml-trn1-instances-model-deployment/)]. Therefore we need to force the region to us-east-2._\n",
    "\n",
    "Lets create our SageMaker session and upload our model to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a37dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952983b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-2-558105141721\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "assert sess.boto_region_name in [\"us-east-2\", \"us-east-1\"] , \"region must be us-east-2 or us-west-2, due to instance availability\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefee504",
   "metadata": {},
   "source": [
    "We create our `model.tar.gz` with our `inference.py` script. \n",
    "\n",
    "_Note: We will use `pigz` for multi-core compression to speed up the process. Make sure `pigz` is installed on your system, you can install it on ubuntu with `sudo apt install pigz`. With `pigz` and 32 cores compression takes ~2.4min_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6b711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/huggingface-inferentia2-samples/llama2-7b/llama_neuron\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/huggingface-inferentia2-samples/llama2-7b\n"
     ]
    }
   ],
   "source": [
    "# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd {save_directory}\n",
    "!tar -cf model.tar.gz --use-compress-program=pigz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ff630",
   "metadata": {},
   "source": [
    "Next, we upload our `model.tar.gz` to Amazon S3 using our session bucket and `sagemaker` sdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6146af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifcats uploaded to s3://sagemaker-us-east-2-558105141721/neuronx/llama/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/neuronx/llama\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=f\"{save_directory}/model.tar.gz\", desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e1395a",
   "metadata": {},
   "source": [
    "## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "\n",
    "After we have uploaded ourÂ model artifactsÂ to Amazon S3 can we create a customÂ `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n",
    "\n",
    "The `inf2.xlarge` instance type is the smallest instance type with AWS Inferentia2 support. It comes with 1 Inferentia2 chip with 2 Neuron Cores. This means we can use 2 Neuron Cores to minimize latency for our image generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41522ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!send warmup request\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,        # path to your model.tar.gz on s3\n",
    "   role=role,                      # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.34.1\",  # transformers version used\n",
    "   pytorch_version=\"1.13.1\",       # pytorch version used\n",
    "   py_version='py310',             # python version used\n",
    "   model_server_workers=1,         # number of workers for the model server\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.8xlarge\", # AWS Inferentia Instance\n",
    "    volume_size = 100\n",
    ")\n",
    "# ignore the \"Your model is not compiled. Please compile your model before using Inferentia.\" warning, we already compiled our model.\n",
    "# We need to sent a warmup request to the endpoint, which loads the model on the neuron device\n",
    "# this takes around 2 minutes\n",
    "print(\"send warmup request\")\n",
    "try: \n",
    "    predictor.predict({\"inputs\": [{\"role\":\"user\",\"content\":\"warmup\"}]})\n",
    "except:\n",
    "    time.sleep(90)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c858560",
   "metadata": {},
   "source": [
    "# 5. Run inference on Llama 2\n",
    "\n",
    "The `.deploy()` returns an `HuggingFacePredictor` object which can be used to request inference. Our endpoint expects a `json` with `messages`. Since we are leveraging the new [apply_chat_template](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) in our inference.py script we can send \"openai\" like converstaions to our model. \n",
    "\n",
    "Additionally we can send inference parameters, e.g. `top_p` or `temperature` using the `parameters` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237f198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: What is Amazon SageMaker?\n",
      "assistant: Amazon SageMaker is a fully managed service provided by Amazon Web Services (AWS) that enables developers and data scientists to build, train, and deploy machine learning (ML) models quickly and easily. It provides a range of tools and features to help users work with ML models, including data preparation, model training, model evaluation, and deployment.\n",
      "\n",
      "SageMaker allows users to work with a variety of ML frameworks, including TensorFlow, PyTorch, and Scikit-learn, and provides integration with other AWS services such as Amazon S3, Amazon Redshift, and Amazon Elastic Compute Cloud (EC2).\n",
      "\n",
      "Some of the key features of Amazon SageMaker include:\n",
      "\n",
      "1. Data Wrangling: SageMaker provides tools for data preparation, including data cleaning, feature engineering, and data transformation.\n",
      "2. Model Training: SageMaker allows users to train ML models using a variety of frameworks and algorithms, including supervised, unsupervised, and reinforcement learning.\n",
      "3. Model Evaluation: SageMaker provides tools for evaluating the performance of ML models, including metrics such as accuracy, precision, and recall.\n",
      "4. Model Deployment: SageMaker allows users to deploy ML models to a variety of environments, including AWS services such as Amazon S3 and Amazon EC2, as well as on-premises environments.\n",
      "5. Automated Machine Learning (AutoML): SageMaker provides automated ML capabilities, including automated model selection, hyperparameter tuning, and model deployment.\n",
      "6. Collaboration: SageMaker provides tools for collaboration, including version control, team management, and collaboration on ML projects.\n",
      "7. Monitoring and Debugging: SageMaker provides tools for monitoring and debugging ML models, including logging, metrics, and visualization.\n",
      "8. Integration with Other AWS Services: SageMaker integrates with other AWS services, such as Amazon S3, Amazon Redshift, and Amazon EC2, to provide a complete ML solution.\n",
      "\n",
      "Amazon SageMaker is a powerful tool for building, training, and deploying ML models, and can be used in a variety of applications, including image and speech recognition, natural language processing, recommendation systems, and more.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI like conversational messages\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"What is Amazon SageMaker?\"},\n",
    "]\n",
    "\n",
    "# generation parameters\n",
    "parameters = {\n",
    "    \"do_sample\" : True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "}\n",
    "\n",
    "# run prediction\n",
    "response = predictor.predict({\n",
    "  \"inputs\": messages,\n",
    "  \"parameters\": parameters\n",
    "  }\n",
    ")\n",
    "\n",
    "# lets our response to the messages and print the generation\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[\"generation\"]})\n",
    "\n",
    "# small helper function to print the messages\n",
    "def pretty_print(messages):\n",
    "    for message in messages:\n",
    "        print(f\"{message['role']}: {message['content']}\")\n",
    "        \n",
    "pretty_print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a467eb",
   "metadata": {},
   "source": [
    "Since Llama is a conversational model lets ask a follow up question. Therefore we can extend our `messages` with a new message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c6035b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: What is Amazon SageMaker?\n",
      "assistant: Amazon SageMaker is a fully managed service provided by Amazon Web Services (AWS) that enables developers and data scientists to build, train, and deploy machine learning (ML) models quickly and easily. It provides a range of tools and features to help users work with ML models, including data preparation, model training, model evaluation, and deployment.\n",
      "\n",
      "SageMaker allows users to work with a variety of ML frameworks, including TensorFlow, PyTorch, and Scikit-learn, and provides integration with other AWS services such as Amazon S3, Amazon Redshift, and Amazon Elastic Compute Cloud (EC2).\n",
      "\n",
      "Some of the key features of Amazon SageMaker include:\n",
      "\n",
      "1. Data Wrangling: SageMaker provides tools for data preparation, including data cleaning, feature engineering, and data transformation.\n",
      "2. Model Training: SageMaker allows users to train ML models using a variety of frameworks and algorithms, including supervised, unsupervised, and reinforcement learning.\n",
      "3. Model Evaluation: SageMaker provides tools for evaluating the performance of ML models, including metrics such as accuracy, precision, and recall.\n",
      "4. Model Deployment: SageMaker allows users to deploy ML models to a variety of environments, including AWS services such as Amazon S3 and Amazon EC2, as well as on-premises environments.\n",
      "5. Automated Machine Learning (AutoML): SageMaker provides automated ML capabilities, including automated model selection, hyperparameter tuning, and model deployment.\n",
      "6. Collaboration: SageMaker provides tools for collaboration, including version control, team management, and collaboration on ML projects.\n",
      "7. Monitoring and Debugging: SageMaker provides tools for monitoring and debugging ML models, including logging, metrics, and visualization.\n",
      "8. Integration with Other AWS Services: SageMaker integrates with other AWS services, such as Amazon S3, Amazon Redshift, and Amazon EC2, to provide a complete ML solution.\n",
      "\n",
      "Amazon SageMaker is a powerful tool for building, training, and deploying ML models, and can be used in a variety of applications, including image and speech recognition, natural language processing, recommendation systems, and more.\n",
      "user: Can I run Hugging Face Transformers on it?\n",
      "assistant: Yes, you can run Hugging Face Transformers on Amazon SageMaker. Hugging Face Transformers is a popular open-source library for natural language processing (NLP) tasks, and it supports a wide range of platforms, including AWS.\n",
      "\n",
      "Here are the general steps to run Hugging Face Transformers on Amazon SageMaker:\n",
      "\n",
      "1. Install the Hugging Face Transformers library on your AWS instance: You can install the library using pip, the Python package manager. You can do this by running the following command in your terminal:\n",
      "```\n",
      "pip install transformers\n",
      "```\n",
      "2. Import the library in your Python code: Once you have installed the library, you can import it in your Python code using the following line of code:\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\n",
      "```\n",
      "3. Load your dataset: You can use the `train.csv` file that comes with the Hugging Face Transformers library to load your dataset. Here's an example of how to do this:\n",
      "```python\n",
      "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
      "encoded_data = tokenizer(train.csv, return_tensors='pt')\n",
      "```\n",
      "4. Create a training argument: You can create a training argument using the `TrainingArguments` class from the Hugging Face Transformers library. Here's an example of how to do this:\n",
      "```python\n",
      "training_args = TrainingArguments(\n",
      "    output_dir='./results',\n",
      "    num_train_epochs=3,\n",
      "    per_device_train_batch_size=16,\n",
      "    per_device_eval_batch_size=64,\n",
      "    evaluation_strategy='epoch',\n",
      "    learning_rate=5e-5,\n",
      "    save_total_limit=2,\n",
      "    save_steps=500,\n",
      "    load_best_model_at_end=True,\n",
      "    metric_for_best_model='accuracy',\n",
      "    greater_is_better=True,\n",
      "    save_strategy='steps',\n",
      "    save_on_each_node=True,\n",
      ")\n",
      "```\n",
      "5. Train your model: You can train your model using the `train` function from the H\n"
     ]
    }
   ],
   "source": [
    "# add follow up question\n",
    "messages.append({\"role\": \"user\", \"content\": \"Can I run Hugging Face Transformers on it?\"})\n",
    "\n",
    "# run prediction\n",
    "response = predictor.predict({\n",
    "  \"inputs\": messages,\n",
    "  \"parameters\": parameters\n",
    "  }\n",
    ")\n",
    "\n",
    "# lets our response to the messages and print the generation\n",
    "messages.append({\"role\": \"assistant\", \"content\": response[\"generation\"]})\n",
    "pretty_print(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1030c87f",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8917d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6cd67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
