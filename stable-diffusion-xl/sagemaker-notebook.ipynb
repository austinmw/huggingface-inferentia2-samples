{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e90d5c",
   "metadata": {},
   "source": [
    "# Deploy Stable Diffusion on AWS inferentia2 with Amazon SageMaker\n",
    "\n",
    "In this end-to-end tutorial, you will learn how to deploy and speed up Stable Diffusion XL inference using AWS Inferentia2 and [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index) on Amazon SageMaker. [Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index) is the interface between the Hugging Face Transformers & Diffusers library and AWS Accelerators including AWS Trainium and AWS Inferentia2. \n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "1. Convert Stable Diffusion XL to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "2. Create a custom `inference.py` script for Stable Diffusion\n",
    "3. Upload the neuron model and inference script to Amazon S3\n",
    "4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "5. Generate images using the deployed model\n",
    "\n",
    "## Quick intro: AWS Inferentia 2\n",
    "\n",
    "[AWS inferentia (Inf2)](https://aws.amazon.com/de/ec2/instance-types/inf2/) are purpose-built EC2 for deep learning (DL) inference workloads. Inferentia 2 is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls), which promises to deliver up to 4x higher throughput and up to 10x lower latency.\n",
    "\n",
    "| instance size | accelerators | Neuron Cores | accelerator memory | vCPU | CPU Memory |\n",
    "| ------------- | ------------ | ------------ | ------------------ | ---- | ---------- |\n",
    "| inf2.xlarge   | 1            | 2            | 32                 | 4    | 16         |\n",
    "| inf2.8xlarge  | 1            | 2            | 32                 | 32   | 128        |\n",
    "| inf2.24xlarge | 6            | 12           | 192                | 96   | 384        |\n",
    "| inf2.48xlarge | 12           | 24           | 384                | 192  | 768        |\n",
    "\n",
    "Additionally, inferentia 2 will support the writing of custom operators in c++ and new datatypes, including `FP8` (cFP8).\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can findÂ [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)Â more about it.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3db68e5",
   "metadata": {},
   "source": [
    "## 1. Convert Stable Diffusion to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "\n",
    "We are going to use the [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index) to compile/convert our model to neuronx. Optimum Neuron provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks. \n",
    "\n",
    "As a first step, we need to install the `optimum-neuron` and other required packages.\n",
    "\n",
    "*Tip: If you are using Amazon SageMaker Notebook Instances or Studio you can go with the `conda_python3` conda kernel.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "# !pip install \"optimum-neuron[neuronx]==0.0.11\"  --upgrade\n",
    "!pip install \"git+https://github.com/huggingface/optimum-neuron.git@main\" diffusers==0.20.2 --upgrade\n",
    "!python -m pip install \"sagemaker>=2.182.0\"  --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0ef431",
   "metadata": {},
   "source": [
    "After we have installed the `optimum-neuron` we can convert load and convert our model.\n",
    "\n",
    "We are going to use the [stabilityai/stable-diffusion-xl-base-1.0](hstabilityai/stable-diffusion-xl-base-1.0) model. Stable Diffusion XL (SDXL) from [Stability AI](https://stability.ai/) is the newset text-to-image generation model, which can create photorealistic images with detailed imagery and composition compared to previous SD models, including SD 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96d8dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "At the time of writing, the [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/dynamic-shapes.html?highlight=dynamic%20shapes#), which means that the we need to specify our image size in advanced for compiling and inference. \n",
    "\n",
    "In simpler terms, this means we need to define the input shapes for our prompt (sequence length), batch size, height and width of the image.\n",
    "\n",
    "_Note: You need to use ~50GB of memory and the compilation can take 10-20 minutes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.neuron import NeuronStableDiffusionXLPipeline\n",
    "\n",
    "# configs for compiling model\n",
    "compiler_args = {\"auto_cast\": \"all\", \"auto_cast_type\": \"bf16\"}\n",
    "input_shapes = {\n",
    "  \"height\": 1024, # width of the image\n",
    "  \"width\": 1024, # height of the image\n",
    "  \"sequence_length\":64, # sequence legnth for the input prompt\n",
    "  \"num_images_per_prompt\": 4 # number of images to generate per prompt\n",
    "  }\n",
    "\n",
    "sd = NeuronStableDiffusionXLPipeline.from_pretrained(model_id, export=True, **input_shapes,**compiler_args)\n",
    "\n",
    "# Save locally or upload to the HuggingFace Hub\n",
    "save_directory = \"sdxl_neuron/\"\n",
    "sd.save_pretrained(save_directory)\n",
    "\n",
    "# COMMENT IN TO UPLOAD TO HUB\n",
    "# sd.push_to_hub(\n",
    "#      save_directory, repository_id=\"my-neuron-repo\", use_auth_token=True\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9997e9db",
   "metadata": {},
   "source": [
    "## 2. Create a custom `inference.py` script for Stable Diffusion\n",
    "\n",
    "The [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) supports zero-code deployments on top of theÂ [pipelineÂ feature](https://huggingface.co/transformers/main_classes/pipelines.html)Â from ðŸ¤— Transformers. This allows users to deploy Hugging Face transformers without an inference script [[Example](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)]. \n",
    "\n",
    "Currently is this feature not supported with AWS Inferentia2, which means we need to provide an `inference.py` for running inference. But `optimum-neuron` has integrated support for the ðŸ¤— Diffusers pipeline feature. That way we can use the `optimum-neuron` to create a pipeline for our model.\n",
    "\n",
    "If you want to know more about the `inference.py`Â script check out this [example](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb). It explains amongst other things what the `model_fn` and `predict_fn` are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create code directory in our model directory\n",
    "!mkdir {save_directory}/code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39f74625",
   "metadata": {},
   "source": [
    "In addition to our `inference.py` script we need to provide a `requirements.txt`, which installs the latest version of the `optimum-neuron` package, which comes with `pipeline` support for AWS Inferentia2. \n",
    "_Note: This is a temporary solution until the `optimum-neuron` package is updated inside the DLC._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {save_directory}/code/requirements.txt\n",
    "optimum-neuron==0.0.11\n",
    "diffusers==0.20.2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce675df9",
   "metadata": {},
   "source": [
    "We are using the `NEURON_RT_NUM_CORES=2` to make sure that each HTTP worker uses 2 Neuron core to maximize throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "# To use two neuron core per worker\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"2\"\n",
    "import torch\n",
    "import torch_neuronx\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from optimum.neuron import NeuronStableDiffusionXLPipeline\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load local converted model into pipeline\n",
    "    pipeline = NeuronStableDiffusionXLPipeline.from_pretrained(model_dir, device_ids=[0, 1])\n",
    "        return pipeline\n",
    "\n",
    "\n",
    "def predict_fn(data, pipeline):\n",
    "    # extract prompt from data\n",
    "    prompt = data.pop(\"inputs\", data)\n",
    "    \n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "    \n",
    "    if parameters is not None:\n",
    "        generated_images = pipeline(inputs, **parameters)[\"images\"]\n",
    "    else:\n",
    "        generated_images = pipeline(inputs)[\"images\"]\n",
    "        \n",
    "    # postprocess convert image into base64 string\n",
    "    encoded_images = []\n",
    "    for image in generated_images:\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        encoded_images.append(base64.b64encode(buffered.getvalue()).decode())\n",
    "\n",
    "    # always return the first \n",
    "    return {\"generated_images\": encoded_images}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## 3. Upload the neuron model and inference script to Amazon S3\n",
    "\n",
    "Before we can deploy our neuron model to Amazon SageMaker we need to upload it all our model artifacts to Amazon S3.\n",
    "\n",
    "_Note: Currently `inf2` instances are only available in the `us-east-2` region [[REF](https://aws.amazon.com/de/about-aws/whats-new/2023/05/sagemaker-ml-inf2-ml-trn1-instances-model-deployment/)]. Therefore we need to force the region to us-east-2._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-2\" # need to set to ohio region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ea3df1",
   "metadata": {},
   "source": [
    "Now lets create our SageMaker session and upload our model to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952983b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "assert sess.boto_region_name == \"us-east-2\", \"region must be us-east-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ff630",
   "metadata": {},
   "source": [
    "Next, we upload our `sdxl_neuron` directory to Amazon S3 using our session bucket and `sagemaker` sdk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/neuronx/{model_id}\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=save_directory ,desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e1395a",
   "metadata": {},
   "source": [
    "## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "\n",
    "After we have uploaded ourÂ model artifactsÂ to Amazon S3 can we create a customÂ `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n",
    "\n",
    "The `inf2.xlarge` instance type is the smallest instance type with AWS Inferentia2 support. It comes with 1 Inferentia2 chip with 2 Neuron Cores. This means we can use 2 Neuron Cores to minimize latency for our image generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41522ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data={\n",
    "        'S3DataSource': {\n",
    "            'CompressionType': 'None',\n",
    "            'S3DataType': 'S3Prefix',\n",
    "            'S3Uri': s3_model_uri,\n",
    "        }\n",
    "    },\n",
    "   role=role,                      # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.28.1\",  # transformers version used\n",
    "   pytorch_version=\"1.13.0\",       # pytorch version used\n",
    "   py_version='py38',              # python version used\n",
    "   model_server_workers=2,         # number of workers for the model server\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.xlarge\" # AWS Inferentia Instance\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c858560",
   "metadata": {},
   "source": [
    "# 5.Generate images using the deployed model\n",
    "\n",
    "The `.deploy()` returns an `HuggingFacePredictor` object which can be used to request inference. Our endpoint expects a `json` with at least `inputs` key. The `inputs` key is the input prompt for the model, which will be used to generate the image. Additionally, we can provide inference parameters, e.g. `num_inference_steps`.\n",
    "\n",
    "The `predictor.predict()` function returns a `json` with the `generated_images` key. The `generated_images` key contains the `4` generated images as a `base64` encoded string. To decode our response we added a small helper function `decode_base64_to_image` which takes the `base64` encoded string and returns a `PIL.Image` object and `display_images`, which takes a list of `PIL.Image` objects and displays them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ff049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper decoder\n",
    "def decode_base64_image(image_string):\n",
    "  base64_image = base64.b64decode(image_string)\n",
    "  buffer = BytesIO(base64_image)\n",
    "  return Image.open(buffer)\n",
    "\n",
    "# display PIL images as grid\n",
    "def display_images(images=None,columns=3, width=100, height=100):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "Now, lets generate some images. As example lets generate `4` images for the prompt `A dog trying catch a flying pizza art drawn by disney concept artists`. Generating `4` images takes around `30` seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_prompt = 3\n",
    "prompt = \"A dog trying catch a flying pizza art drawn by disney concept artists, golden colour, high quality, highly detailed, elegant, sharp focus\"\n",
    "\n",
    "# run prediction\n",
    "response = predictor.predict(data={\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"num_inference_steps\" : 25,\n",
    "    } \n",
    "  }\n",
    ")\n",
    "\n",
    "# decode images\n",
    "decoded_images = [decode_base64_image(image) for image in response[\"generated_images\"]]\n",
    "\n",
    "# visualize generation\n",
    "display_images(decoded_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1030c87f",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8917d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656e81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
