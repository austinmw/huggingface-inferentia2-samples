{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Llama 2 70B on AWS Inferentia2 with Hugging Face Optimum\n",
    "\n",
    "\n",
    "[Llama 2](meta-llama/Llama-2-70b-chat-hf) is the latest is the open LLM from Meta, released in July 2023. It is trained on more data - 2T tokens and supports context length window upto 4K tokens and still one of the best open available LLMs. Meta fine-tuned conversational models with Reinforcement Learning from Human Feedback on over 1 million human annotations.\n",
    "\n",
    "In this blog you will learn how to deploy [meta-llama/Llama-2-70b-chat-hf](meta-llama/Llama-2-70b-chat-hf) model on AWS Inferentia2 with Hugging Face Optimum on Amazon SageMaker. We are going to use the Hugging Face LLM Inf2 Container a new purpose-built Inference Container to easily deploy LLMs on AWS Inferentia2 powered by[ Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index) and [Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index).\n",
    "\n",
    "\n",
    "In the blog will cover how to:\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Retrieve the new Hugging Face LLM Inf2 DLC](#2-retrieve-the-new-hugging-face-llm-inf2-dlc)\n",
    "3. [Deploy Llama 2 70B to inferentia2](#3-deploy-llama-2-70b-to-inferentia2)\n",
    "4. [Run inference and chat with the model](#4-run-inference-and-chat-with-the-model)\n",
    "5. [Benchmark Llama 2 70B on inferentia2](#5-benchmark-llama-2-70b-on-inferentia2)\n",
    "6. [Clean up](#6-clean-up)\n",
    "\n",
    "Lets get started! ðŸš€\n",
    "\n",
    "## Quick intro: AWS Inferentia 2\n",
    "\n",
    "[AWS inferentia (Inf2)](https://aws.amazon.com/de/ec2/instance-types/inf2/) are purpose-built EC2 for deep learning (DL) inference workloads. Inferentia 2 is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls), which promises to deliver up to 4x higher throughput and up to 10x lower latency.\n",
    "\n",
    "| instance size | accelerators | Neuron Cores | accelerator memory | vCPU | CPU Memory | on-demand price ($/h) |\n",
    "| ------------- | ------------ | ------------ | ------------------ | ---- | ---------- | --------------------- |\n",
    "| inf2.xlarge   | 1            | 2            | 32                 | 4    | 16         | 0.76                  |\n",
    "| inf2.8xlarge  | 1            | 2            | 32                 | 32   | 128        | 1.97                  |\n",
    "| inf2.24xlarge | 6            | 12           | 192                | 96   | 384        | 6.49                  |\n",
    "| inf2.48xlarge | 12           | 24           | 384                | 192  | 768        | 12.98                 |\n",
    "\n",
    "Additionally, inferentia 2 will support the writing of custom operators in c++ and new datatypes, including `FP8` (cFP8).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy Mixtral to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.199.0\" gradio transformers --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve the new Hugging Face LLM Inf2 DLC\n",
    "\n",
    "The new Hugging Face TGI Neuronx DLCs can be used to run inference on AWS Inferentia2. You can use the `get_huggingface_llm_image_uri` method of the `sagemaker` SDK to retrieve the appropriate Hugging Face TGI Neuronx DLC URI based on your desired `backend`, `session`, `region`, and `version`. You can find all the available versions [here](https://github.com/aws/deep-learning-containers/releases?q=tgi+AND+neuronx&expanded=true).\n",
    "\n",
    "*Note: At the time of writing this blog post the latest version of the Hugging Face LLM DLC is not yet available via the `get_huggingface_llm_image_uri` method. We are going to use the raw container uri instead.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface-neuronx\",\n",
    "  version=\"0.0.20\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"1.4.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy Llama 2 70B to inferentia2\n",
    "\n",
    "At the time of writing, [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/v2.6.0/general/arch/neuron-features/dynamic-shapes.html#neuron-dynamic-shapes), which means that we need to specify our sequence length and batch size ahead of time.\n",
    "To make it easier for customers to utilize the full power of Inferentia2, we created a [neuron model cache](https://huggingface.co/docs/optimum-neuron/guides/cache_system), which contains pre-compiled configurations for the most popular LLMs, including Llama 2 70B. \n",
    "\n",
    "This means we don't need to compile the model ourselves, but we can use the pre-compiled model from the cache. You can find compiled/cached configurations on the [Hugging Face Hub](https://huggingface.co/aws-neuron/optimum-neuron-cache/tree/main/inference-cache-config). If your desired configuration is not yet cached, you can compile it yourself using the [Optimum CLI](https://huggingface.co/docs/optimum-neuron/cli/compile) or open a request at the [Cache repository](https://huggingface.co/aws-neuron/optimum-neuron-cache/discussions)\n",
    "\n",
    "Below is an example on how to compile Llama 2 70B with Optimum CLI, thats not needed in this case as we are using the pre-compiled model from the cache.\n",
    "\n",
    "**Example: Compile Llama 2 70B with Optimum CLI**\n",
    "\n",
    "```bash\n",
    "# login into the huggingface hub to access gated models, like llama\n",
    "huggingface-cli login --token [API_TOKEN]\n",
    "# compile model with optimum for batch size 4 and sequence length 2048\n",
    "optimum-cli export neuron -m meta-llama/Llama-2-70b-chat-hf --batch_size 4 --sequence_length 2048 --num_cores 24 --auto_cast_type fp16 ./llama-70b-chat-neuron\n",
    "# push model to hub [repo_id] [local_path] [path_in_repo]\n",
    "huggingface-cli upload  aws-neuron/Llama-2-70b-chat-seqlen-2048-bs-4 ./llama-70b-chat-neuron ./ --exclude \"checkpoint/**\"\n",
    "# Move tokenizer to neuron model repository\n",
    "python -c \"from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('meta-llama/Llama-2-70b-chat-hf').push_to_hub('aws-neuron/Llama-2-70b-chat-seqlen-2048-bs-4')\"\n",
    "```\n",
    "\n",
    "_Note: You need to compile models on an AWS EC2 instance with Inferentia2 support. Compilation can take up to 45 minutes._\n",
    "\n",
    "**Deploying Llama2 70B as Endpoint**  \n",
    "\n",
    "Before deploying the model to Amazon SageMaker, we must define the TGI Neuronx endpoint configuration. We need to make sure the following additional parameters are defined: \n",
    "\n",
    "- `HF_NUM_CORES`: Number of Neuron Cores used for the compilation.\n",
    "- `HF_BATCH_SIZE`: The batch size that was used to compile the model.\n",
    "- `HF_SEQUENCE_LENGTH`: The sequence length that was used to compile the model.\n",
    "- `HF_AUTO_CAST_TYPE`: The auto cast type that was used to compile the model.\n",
    "\n",
    "We still need to define traditional TGI parameters with:\n",
    "\n",
    "- `HF_MODEL_ID`: The Hugging Face model ID.\n",
    "- `HF_TOKEN`: The Hugging Face API token to access gated models.\n",
    "- `MAX_BATCH_SIZE`: The maximum batch size that the model can handle, equal to the batch size used for compilation.\n",
    "- `MAX_INPUT_LENGTH`: The maximum input length that the model can handle. \n",
    "- `MAX_TOTAL_TOKENS`: The maximum total tokens the model can generate, equal to the sequence length used for compilation.\n",
    "\n",
    "**Select the right instance type**\n",
    "\n",
    "Llama 2 70B is a large model and requires a lot of memory. We are going to use the `inf2.48xlarge` instance type, which has 192 vCPUs and 384 GB of accelerator memory. The `inf2.48xlarge` instance comes with 12 Inferentia2 accelerators that include 24 Neuron Cores. If you want to find the cached configurations for Llama 2 70B, you can find them [here](https://huggingface.co/aws-neuron/optimum-neuron-cache/blob/6392789bb1f2ab2af2867f0ab564058b781054ca/inference-cache-config/llama.json#L102). In our case we will use a batch size of 4 and a sequence length of 4096. \n",
    "\n",
    "\n",
    "Before we can deploy Llama 2 70B to Inferentia2, we need to make sure we are logged in to the Hugging Face Hub and have the necessary permissions to access the model. You can request access to the model [here](https://huggingface.co/meta-llama/Llama-2-70b-chat-hf).\n",
    "\n",
    "```bash\n",
    "huggingface-cli login --token [API_TOKEN]\n",
    "```\n",
    "\n",
    "After that we can create our endpoint configuration and deploy the model to Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.inf2.48xlarge\"\n",
    "health_check_timeout=2400 # additional time to load the model\n",
    "volume_size=512 # size in GB of the EBS volume\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"meta-llama/Llama-2-70b-chat-hf\",\n",
    "    \"HF_NUM_CORES\": \"24\", # number of neuron cores\n",
    "    \"HF_BATCH_SIZE\": \"4\", # batch size used to compile the model\n",
    "    \"HF_SEQUENCE_LENGTH\": \"4096\", # length used to compile the model\n",
    "    \"HF_AUTO_CAST_TYPE\": \"fp16\",  # dtype of the model\n",
    "    \"MAX_BATCH_SIZE\": \"4\", # max batch size for the model\n",
    "    \"MAX_INPUT_LENGTH\": \"3686\", # max length of input text\n",
    "    \"MAX_TOTAL_TOKENS\": \"4096\", # max length of generated text\n",
    "    \"HF_TOKEN\": HfFolder.get_token(), # pass the huggingface token\n",
    "}\n",
    "\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.g5.48xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,\n",
    "  volume_size=volume_size\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 20-30 minutes, we are working on improving the deployment time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run inference and chat with the model\n",
    "\n",
    "After our endpoint is deployed, we can run inference on it, using the `predict` method from `predictor`. We can provide different parameters to impact the generation, adding them to the `parameters` attribute of the payload. You can find the supported parameters [here](https://www.philschmid.de/sagemaker-llama-llm#5-run-inference-and-chat-with-the-model), or in the open API specification of TGI in the [swagger documentation](https://huggingface.github.io/text-generation-inference/)\n",
    "\n",
    "The `meta-llama/Llama-2-70b-chat-hf` is a conversational chat model, meaning we can chat with it using a prompt structure like the following:\n",
    "\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{{ system_prompt }}\n",
    "<</SYS>>\n",
    "\n",
    "{{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST] {{ model_answer_2 }} </s><s>[INST] {{ user_msg_3 }} [/INST]\n",
    "```\n",
    "\n",
    "\n",
    "Manually preparing the prompt is error prone, so we can use the `apply_chat_template` method from the tokenizer to help with it. It expects a `messages` dictionary in the well-known OpenAI format, and converts it into the correct format for the model. Let's see if Llama 2 knows some facts about AWS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-70b-chat-hf\")\n",
    "\n",
    "# Prompt to generate\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are the AWS expert\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me an interesting fact about AWS?\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"return_full_text\": False,\n",
    "}\n",
    "\n",
    "res = llm.predict({\"inputs\": prompt, \"parameters\": parameters})\n",
    "print(res[0][\"generated_text\"].strip().replace(\"</s>\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we tested infernece now lets build a cool demo which support streaming responses. [Amazon SageMaker supports streaming responses](https://aws.amazon.com/de/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/) from your model. We can use this to stream responses, we can leverage this to create a streaming gradio application with a better user experience.\n",
    "\n",
    "We created a sample application that you can use to test your model. You can find the code in [gradio-app.py](../demo/sagemaker_chat.py). The application will stream the responses from the model and display them in the UI. You can also use the application to test your model with your own inputs. With `share=True` you can share the application with others, since gradio with create a public link for you valid for 72 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add apps directory to path ../apps/\n",
    "import sys\n",
    "sys.path.append(\"../demo\") \n",
    "from sagemaker_chat import create_gradio_app\n",
    "\n",
    "# create gradio app\n",
    "create_gradio_app(\n",
    "    llm.endpoint_name,           # Sagemaker endpoint name\n",
    "    session=sess.boto_session,   # boto3 session used to send request \n",
    "    system_prompt=\"You are an helpful Assistant, called Llama 2. Knowing everyting about AWS.\",\n",
    "    tokenizer=tokenizer,         # Tokenizer to use format prompt\n",
    "    concurrency_count=4,         # Number of concurrent requests\n",
    "    share=True,                  # Share app publicly\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gradio](./imgs/gradio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark Llama 2 70B on inferentia2\n",
    "\n",
    "In the last step we are going to benchmark the model on Inferentia2. We are going to run a simple load test where we send multiple parallel requests to the model and measure the latency and throughput of the model. \n",
    "\n",
    "We added a `utils` helper to retrieve metrics from the cloudwatch logs, but this still includes Network latency and other overheads. We are working on a detailed, reproducible benchmarking guide for Optimum Neuron models on Inferentia2. Stay tuned! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm \n",
    "import json \n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from get_metrics import get_metrics_from_cloudwatch\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_new_tokens\": 250,\n",
    "    \"return_full_text\": False,\n",
    "}\n",
    "\n",
    "# The function to perform a single request\n",
    "def make_request(payload):\n",
    "    try: \n",
    "        llm.predict(\n",
    "            data={\n",
    "                \"inputs\": tokenizer.apply_chat_template(\n",
    "                    [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": payload\n",
    "                        }\n",
    "                    ],\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True,\n",
    "                ),\n",
    "                \"parameters\": parameters,\n",
    "            }\n",
    "        )\n",
    "        return 200\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 500\n",
    "\n",
    "# Main function to run the load test\n",
    "def run_load_test(total_requests, concurrent_users):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "        # Prepare a list of the same inputs to hit multiple times\n",
    "        tasks = [\"Write a long story about llamas and why should protect them.\"] * total_requests\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # run the requests\n",
    "        results = list(tqdm(executor.map(make_request, tasks), total=total_requests, desc=\"Running load test\"))\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"Total time for {total_requests} requests with {concurrent_users} concurrent users: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Successful rate: {results.count(200) / total_requests * 100:.2f}%\")\n",
    "        # Get the metrics\n",
    "        metrics = get_metrics_from_cloudwatch(   \n",
    "            endpoint_name=llm.endpoint_name,\n",
    "            st=int(start_time),\n",
    "            et=int(end_time),\n",
    "            cu=concurrent_users,\n",
    "            total_requests=total_requests,\n",
    "            boto3_session=sess.boto_session\n",
    "        )\n",
    "        # store results\n",
    "        with open(\"results.json\", \"w\") as f:\n",
    "            json.dump(metrics, f)\n",
    "        # print results\n",
    "        print(f\"Llama 2 70B results on `inf2.48xlarge`:\")\n",
    "        print(f\"Throughput: {metrics['Thorughput (tokens/second)']:,.2f} tokens/s\")\n",
    "        print(f\"Latency p(50): {metrics['Latency (ms/token) p(50)']:,.2f} ms/token\")\n",
    "        return metrics\n",
    "\n",
    "# Run the load test\n",
    "concurrent_users = 5\n",
    "number_of_requests = 100\n",
    "res = run_load_test(number_of_requests, concurrent_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: We want to mention again that the benchmark is not a perfect representation of the model performance, as it includes network latency and other overheads. We were sending request from eu-central-1 to us-east-1, which adds additional latency to the requests. We are working on a detailed benchmark with more metrics and a better understanding of the model performance._\n",
    "\n",
    "Results with 250 token generation: \n",
    "```bash\n",
    "Llama 2 70B results on inf2.48xlarge:\n",
    "Throughput: 42.23 tokens/s\n",
    "Latency p(50): 88.80 ms/token\n",
    "```\n",
    "\n",
    "_Note: We ran a similar test on a `ml.g5.48xlarge` instance with 8 NVIDIA A10G GPUs as well, but needed to decrease the context size to 2048 from 4096 and generated tokens from 250 to 50. With g5 we achieved an thorughput of `~38.5` tokens per second._\n",
    "\n",
    "On Inferentia2 we achieved a throughput of `~42.23` tokens per second. Thats a improvement compared to the `ml.g5.48xlarge` instance. The `g5` benchmark was slightly different so it is not 100% compareable.\n",
    "Assuming it would be the cost-performance per token between `g5.48xlarge` (`16.28$/h`) and `inf2.48xlarge` (`12,98$`) would show a ~44.7% improvement for $ per token price. This makes inferentia2 a valid alternative to NVIDIA A10G GPUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
