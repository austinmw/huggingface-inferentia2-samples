{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e90d5c",
   "metadata": {},
   "source": [
    "# Deploy Embedding Models on AWS inferentia2 with Amazon SageMaker\n",
    "\n",
    "In this end-to-end tutorial, you will learn how to deploy and speed up Embeddings Model inference using AWS Inferentia2 and [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index) on Amazon SageMaker. [Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index) is the interface between the Hugging Face Transformers & Diffusers library and AWS Accelerators including AWS Trainium and AWS Inferentia2. \n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "1. Convert Embeddings Model to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "2. Create a custom `inference.py` script for `embeddings`\n",
    "3. Upload the neuron model and inference script to Amazon S3\n",
    "4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "5. Run and evaluate Inference performance of Embeddings Model on Inferentia2\n",
    "\n",
    "Let's get started! 🚀\n",
    "\n",
    "---\n",
    "\n",
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3db68e5",
   "metadata": {},
   "source": [
    "## 1. Convert BERT to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "\n",
    "We are going to use the [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index). 🤗 Optimum Neuron is the interface between the 🤗 Transformers library and AWS Accelerators including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks. \n",
    "\n",
    "As a first step, we need to install the `optimum-neuron` and other required packages.\n",
    "\n",
    "*Tip: If you are using Amazon SageMaker Notebook Instances or Studio you can go with the `conda_python3` conda kernel.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting optimum-neuron[neuronx]==0.0.14\n",
      "  Downloading optimum_neuron-0.0.14-py3-none-any.whl (250 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.1/250.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<=1.25.2,>=1.22.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron[neuronx]==0.0.14) (1.24.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron[neuronx]==0.0.14) (0.16.4)\n",
      "Requirement already satisfied: accelerate==0.23.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron[neuronx]==0.0.14) (0.23.0)\n",
      "Requirement already satisfied: protobuf<4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron[neuronx]==0.0.14) (3.20.1)\n",
      "Requirement already satisfied: optimum>=1.13.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron[neuronx]==0.0.14) (1.13.2)\n",
      "Collecting transformers==4.35.0\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m176.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers-neuronx==0.8.268\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/transformers-neuronx/transformers_neuronx-0.8.268-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m697.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision==0.14.* in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron[neuronx]==0.0.14) (0.14.1)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron[neuronx]==0.0.14) (0.37.1)\n",
      "Requirement already satisfied: torch==1.13.1.* in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron[neuronx]==0.0.14) (1.13.1)\n",
      "Collecting torch-neuronx==1.13.1.1.12.1\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/torch-neuronx/torch_neuronx-1.13.1.1.12.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting neuronx-distributed==0.5.0\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/neuronx-distributed/neuronx_distributed-0.5.0-py3-none-linux_x86_64.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting neuronx-cc==2.11.0.34\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/neuronx-cc/neuronx_cc-2.11.0.34%2Bc5231f848-cp39-cp39-linux_x86_64.whl (371.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.8/371.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from accelerate==0.23.0->optimum-neuron[neuronx]==0.0.14) (5.9.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from accelerate==0.23.0->optimum-neuron[neuronx]==0.0.14) (23.1)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from accelerate==0.23.0->optimum-neuron[neuronx]==0.0.14) (6.0)\n",
      "Requirement already satisfied: islpy<=2023.1,>2021.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (2022.1.1)\n",
      "Collecting ec2-metadata<=2.10.0\n",
      "  Downloading ec2_metadata-2.10.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: pgzip>=0.3.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (0.3.4)\n",
      "Requirement already satisfied: networkx<=2.6.3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (2.6.3)\n",
      "Requirement already satisfied: requests-unixsocket>=0.1.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (0.3.0)\n",
      "Requirement already satisfied: scipy<=1.11.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (1.11.1)\n",
      "Collecting neuronx-hwm==2.11.0.2\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/neuronx-hwm/neuronx_hwm-2.11.0.2%2Be34678757-cp39-cp39-linux_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-daemon>=2.2.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (3.0.1)\n",
      "Requirement already satisfied: torch-xla in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (1.13.0+torchneuron5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch==1.13.1.*->optimum-neuron[neuronx]==0.0.14) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch==1.13.1.*->optimum-neuron[neuronx]==0.0.14) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch==1.13.1.*->optimum-neuron[neuronx]==0.0.14) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch==1.13.1.*->optimum-neuron[neuronx]==0.0.14) (11.10.3.66)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch==1.13.1.*->optimum-neuron[neuronx]==0.0.14) (4.7.1)\n",
      "Collecting libneuronxla==0.5.570\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/libneuronxla/libneuronxla-0.5.570-py3-none-linux_x86_64.whl (52.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.9/52.9 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch-xla\n",
      "  Downloading https://pip.repos.neuron.amazonaws.com/torch-xla/torch_xla-1.13.1%2Btorchneuronc-cp39-cp39-linux_x86_64.whl (267.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torchvision==0.14.*->optimum-neuron[neuronx]==0.0.14) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torchvision==0.14.*->optimum-neuron[neuronx]==0.0.14) (9.2.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers==4.35.0->optimum-neuron[neuronx]==0.0.14) (0.3.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers==4.35.0->optimum-neuron[neuronx]==0.0.14) (3.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers==4.35.0->optimum-neuron[neuronx]==0.0.14) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers==4.35.0->optimum-neuron[neuronx]==0.0.14) (0.14.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers==4.35.0->optimum-neuron[neuronx]==0.0.14) (4.64.1)\n",
      "Requirement already satisfied: aws-neuronx-runtime-discovery~=2.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from libneuronxla==0.5.570->torch-neuronx==1.13.1.1.12.1->optimum-neuron[neuronx]==0.0.14) (2.9)\n",
      "Requirement already satisfied: boto3~=1.26 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from libneuronxla==0.5.570->torch-neuronx==1.13.1.1.12.1->optimum-neuron[neuronx]==0.0.14) (1.28.57)\n",
      "Requirement already satisfied: botocore~=1.29 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from libneuronxla==0.5.570->torch-neuronx==1.13.1.1.12.1->optimum-neuron[neuronx]==0.0.14) (1.31.61)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1.*->optimum-neuron[neuronx]==0.0.14) (67.7.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (1.3.0)\n",
      "Requirement already satisfied: cloud-tpu-client>=0.10.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (0.10)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->optimum-neuron[neuronx]==0.0.14) (2022.8.2)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (4.34.0)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (2.13.0)\n",
      "Requirement already satisfied: coloredlogs in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (1.11.1)\n",
      "Requirement already satisfied: pytest>=2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from islpy<=2023.1,>2021.1->neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (7.1.3)\n",
      "Requirement already satisfied: docutils in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from python-daemon>=2.2.4->neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (0.20)\n",
      "Requirement already satisfied: lockfile>=0.10 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from python-daemon>=2.2.4->neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests->torchvision==0.14.*->optimum-neuron[neuronx]==0.0.14) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests->torchvision==0.14.*->optimum-neuron[neuronx]==0.0.14) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests->torchvision==0.14.*->optimum-neuron[neuronx]==0.0.14) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests->torchvision==0.14.*->optimum-neuron[neuronx]==0.0.14) (3.4)\n",
      "Collecting transformers[sentencepiece]>=4.26.0\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m161.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.35.1-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m170.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers==4.35.0->optimum-neuron[neuronx]==0.0.14) (0.1.97)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from coloredlogs->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (10.0)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (9.0.0)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (3.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (0.3.6)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (3.8.3)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (2.0.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sympy->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (1.2.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3~=1.26->libneuronxla==0.5.570->torch-neuronx==1.13.1.1.12.1->optimum-neuron[neuronx]==0.0.14) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3~=1.26->libneuronxla==0.5.570->torch-neuronx==1.13.1.1.12.1->optimum-neuron[neuronx]==0.0.14) (0.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from botocore~=1.29->libneuronxla==0.5.570->torch-neuronx==1.13.1.1.12.1->optimum-neuron[neuronx]==0.0.14) (2.8.2)\n",
      "Requirement already satisfied: oauth2client in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (4.1.3)\n",
      "Requirement already satisfied: google-api-python-client==1.8.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (1.8.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (0.22.0)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (1.34.0)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (1.16.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (0.1.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (2.14.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (1.8.1)\n",
      "Requirement already satisfied: iniconfig in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=2->islpy<=2023.1,>2021.1->neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (1.1.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=2->islpy<=2023.1,>2021.1->neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (1.11.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=2->islpy<=2023.1,>2021.1->neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pytest>=2->islpy<=2023.1,>2021.1->neuronx-cc==2.11.0.34->optimum-neuron[neuronx]==0.0.14) (2.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (2022.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->datasets->optimum>=1.13.0->optimum-neuron[neuronx]==0.0.14) (2023.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from oauth2client->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (4.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (1.58.0)\n",
      "Collecting protobuf<4\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (5.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client>=0.10.0->torch-xla->neuronx-distributed==0.5.0->optimum-neuron[neuronx]==0.0.14) (3.0.9)\n",
      "Installing collected packages: protobuf, neuronx-hwm, ec2-metadata, neuronx-cc, transformers, libneuronxla, torch-xla, torch-neuronx, optimum-neuron, transformers-neuronx, neuronx-distributed\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: neuronx-hwm\n",
      "    Found existing installation: neuronx-hwm 2.6.0.0+826e77395\n",
      "    Uninstalling neuronx-hwm-2.6.0.0+826e77395:\n",
      "      Successfully uninstalled neuronx-hwm-2.6.0.0+826e77395\n",
      "  Attempting uninstall: neuronx-cc\n",
      "    Found existing installation: neuronx-cc 2.6.0.19+3d819e565\n",
      "    Uninstalling neuronx-cc-2.6.0.19+3d819e565:\n",
      "      Successfully uninstalled neuronx-cc-2.6.0.19+3d819e565\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.0\n",
      "    Uninstalling transformers-4.34.0:\n",
      "      Successfully uninstalled transformers-4.34.0\n",
      "  Attempting uninstall: libneuronxla\n",
      "    Found existing installation: libneuronxla 0.5.205\n",
      "    Uninstalling libneuronxla-0.5.205:\n",
      "      Successfully uninstalled libneuronxla-0.5.205\n",
      "  Attempting uninstall: torch-xla\n",
      "    Found existing installation: torch-xla 1.13.0+torchneuron5\n",
      "    Uninstalling torch-xla-1.13.0+torchneuron5:\n",
      "      Successfully uninstalled torch-xla-1.13.0+torchneuron5\n",
      "  Attempting uninstall: torch-neuronx\n",
      "    Found existing installation: torch-neuronx 1.13.0.1.6.1\n",
      "    Uninstalling torch-neuronx-1.13.0.1.6.1:\n",
      "      Successfully uninstalled torch-neuronx-1.13.0.1.6.1\n",
      "  Attempting uninstall: optimum-neuron\n",
      "    Found existing installation: optimum-neuron 0.0.13\n",
      "    Uninstalling optimum-neuron-0.0.13:\n",
      "      Successfully uninstalled optimum-neuron-0.0.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "hf-endpoints-emulator 0.1.0 requires tensorflow>=2.4.0, which is not installed.\n",
      "onnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "apache-beam 2.49.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ec2-metadata-2.10.0 libneuronxla-0.5.570 neuronx-cc-2.11.0.34+c5231f848 neuronx-distributed-0.5.0 neuronx-hwm-2.11.0.2+e34678757 optimum-neuron-0.0.14 protobuf-3.20.3 torch-neuronx-1.13.1.1.12.1 torch-xla-1.13.1+torchneuronc transformers-4.35.0 transformers-neuronx-0.8.268\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting sagemaker==2.192.0\n",
      "  Downloading sagemaker-2.192.0.tar.gz (902 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m902.7/902.7 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (1.28.57)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (1.24.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (3.20.3)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (23.1)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (2.0.3)\n",
      "Requirement already satisfied: pathos in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (4.16.0)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (3.10.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.192.0) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.57 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.192.0) (1.31.61)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.192.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.192.0) (0.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.192.0) (3.10.0)\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from google-pasta->sagemaker==2.192.0) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from jsonschema->sagemaker==2.192.0) (0.18.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->sagemaker==2.192.0) (2022.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->sagemaker==2.192.0) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->sagemaker==2.192.0) (2.8.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker==2.192.0) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker==2.192.0) (0.70.14)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker==2.192.0) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker==2.192.0) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from schema->sagemaker==2.192.0) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from botocore<1.32.0,>=1.31.57->boto3<2.0,>=1.26.131->sagemaker==2.192.0) (1.26.12)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.192.0-py2.py3-none-any.whl size=1206219 sha256=2435444d9912f3021db21342977bc512684e599d04d8e98a01415e377882df5a\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/80/8a/b4/de2c4b85adf2edde0b4594985b262786c658e8f94d6a84ec4c\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.197.0\n",
      "    Uninstalling sagemaker-2.197.0:\n",
      "      Successfully uninstalled sagemaker-2.197.0\n",
      "Successfully installed sagemaker-2.192.0\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install \"optimum-neuron[neuronx]==0.0.14\"  --upgrade\n",
    "\n",
    "!python -m pip install \"sagemaker==2.192.0\"  --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0ef431",
   "metadata": {},
   "source": [
    "After we have installed the `optimum-neuron` we can convert load and convert our model.\n",
    "\n",
    "We are going to use the [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) model. BGE Base is a fine-tuned BERT model to map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. It works perfectly for vector databases for LLMs. The base model is the perfect trade-off between size and performance, it is currently ranked top 5 on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "\n",
    "At the time of writing, the [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/dynamic-shapes.html?highlight=dynamic%20shapes#), which means that the input size needs to be static for compiling and inference. \n",
    "\n",
    "In simpler terms, this means we need to define the input shapes for our prompt (sequence length), batch size, height and width of the image.\n",
    "\n",
    "We precompiled the model with the following parameters and pushed it to the Hugging Face Hub: \n",
    "* `sequence_length`: 384\n",
    "* `batch_size`: 1\n",
    "* `neuron`: 2.15.0\n",
    "\n",
    "_Note: If you want to compile your own model, comment in the code below and change the model id. We used an `inf2.8xlarge` ec2 instance with the [Hugging Face Neuron Deep Learning AMI](https://aws.amazon.com/marketplace/pp/prodview-gr3e6yiscria2) to compile the model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "\n",
    "# # compiled model id\n",
    "# compiled_model_id = \"aws-neuron/bge-base-en-v1.5-seqlen-384-bs-1\"\n",
    "\n",
    "# # save compiled model to local directory\n",
    "# save_directory = \"embedding_model\"\n",
    "# # Downloads our compiled model from the HuggingFace Hub \n",
    "# # using the revision as neuron version reference\n",
    "# # and makes sure we exlcude the symlink files and \"hidden\" files, like .DS_Store, .gitignore, etc.\n",
    "# snapshot_download(compiled_model_id, revision=\"2.15.0\", local_dir=save_directory, local_dir_use_symlinks=False, allow_patterns=[\"[!.]*.*\"])\n",
    "\n",
    "\n",
    "###############################################\n",
    "# COMMENT IN BELOW TO COMPILE DIFFERENT MODEL #\n",
    "###############################################\n",
    "\n",
    "from optimum.neuron import NeuronModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model id you want to compile\n",
    "vanilla_model_id = \"BAAI/bge-base-en-v1.5\"\n",
    "\n",
    "# configs for compiling model\n",
    "input_shapes = {\n",
    "  \"sequence_length\": 384, # max length of the document (max 512)\n",
    "  \"batch_size\": 1 # batch size for the model\n",
    "  }\n",
    "\n",
    "emb = NeuronModelForSequenceClassification.from_pretrained(vanilla_model_id, export=True, **input_shapes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(vanilla_model_id)\n",
    "\n",
    "# Save locally or upload to the HuggingFace Hub\n",
    "save_directory = \"embedding_model\"\n",
    "emb.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9997e9db",
   "metadata": {},
   "source": [
    "## 2. Create a custom `inference.py` script for `embeddings`\n",
    "\n",
    "The [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) supports zero-code deployments on top of the [pipeline feature](https://huggingface.co/transformers/main_classes/pipelines.html) from 🤗 Transformers. This allows users to deploy Hugging Face transformers without an inference script [[Example](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)]. \n",
    "\n",
    "Currently is this feature not supported with AWS Inferentia2, which means we need to provide an `inference.py` for running inference. But `optimum-neuron` has integrated support for the 🤗 Transformers pipeline feature. That way we can use the `optimum-neuron` to create a pipeline for our model.\n",
    "\n",
    "If you want to know more about the `inference.py` script check out this [example](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb). It explains amongst other things what the `model_fn` and `predict_fn` are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce675df9",
   "metadata": {},
   "source": [
    "We are using the `NEURON_RT_NUM_CORES=1` to make sure that each HTTP worker uses 1 Neuron core to maximize throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "# To use one neuron core per worker\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"1\"\n",
    "from optimum.neuron.pipelines import pipeline\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load local converted model into pipeline\n",
    "    pipe = pipeline(\"feature-extraction\", model=model_dir)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_fn(data, pipeline):\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "\n",
    "    # pass inputs with all kwargs in data\n",
    "    if parameters is not None:\n",
    "        model_output = pipeline(inputs, **parameters)\n",
    "    else:\n",
    "        model_output = pipeline(inputs)\n",
    "    \n",
    "    # Perform pooling. In this case, cls pooling.\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "    # normalize embeddings\n",
    "    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)    \n",
    "    \n",
    "    return {\"embeddings\":sentence_embeddings[0].tolist()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## 3. Upload the neuron model and inference script to Amazon S3\n",
    "\n",
    "Before we can deploy our neuron model to Amazon SageMaker we need to create a `model.tar.gz` archive with all our model artifacts saved into, e.g. `model.neuron` and upload this to Amazon S3.\n",
    "\n",
    "To do this we need to set up our permissions. Currently `inf2` instances are only available in the `us-east-2` region [[REF](https://aws.amazon.com/de/about-aws/whats-new/2023/05/sagemaker-ml-inf2-ml-trn1-instances-model-deployment/)]. Therefore we need to force the region to us-east-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d016feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-2\" # need to set to ohio region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ea3df1",
   "metadata": {},
   "source": [
    "Now lets create our SageMaker session and upload our model to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952983b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "assert sess.boto_region_name == \"us-east-2\", \"region must be us-east-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ff630",
   "metadata": {},
   "source": [
    "Next, we create our `model.tar.gz`.The `inference.py` script will be placed into a `code/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3808b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy inference.py into the code/ directory of the model directory.\n",
    "!cp -r code/ tmp/code/\n",
    "# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd tmp\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6f330",
   "metadata": {},
   "source": [
    "Now we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/neuronx/{model_id}\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tmp directory after uploading\n",
    "# !rm -rf tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e1395a",
   "metadata": {},
   "source": [
    "## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "\n",
    "After we have uploaded our `model.tar.gz` to Amazon S3 can we create a custom `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n",
    "\n",
    "The `inf2.xlarge` instance type is the smallest instance type with AWS Inferentia2 support. It comes with 1 Inferentia2 chip with 2 Neuron Cores. This means we can use 2 Model server workers to maximize throughput and run 2 inferences in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41522ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,        # path to your model and script\n",
    "   role=role,                      # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.28.1\",  # transformers version used\n",
    "   pytorch_version=\"1.13.0\",       # pytorch version used\n",
    "   py_version='py38',              # python version used\n",
    "   model_server_workers=2,         # number of workers for the model server\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.xlarge\" # AWS Inferentia Instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c858560",
   "metadata": {},
   "source": [
    "# 5. Run and evaluate Inference performance of BERT on Inferentia\n",
    "\n",
    "The `.deploy()` returns an `HuggingFacePredictor` object which can be used to request inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ff049",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"inputs\": \"the mesmerizing performances of the leads keep the film grounded and keep the audience riveted .\",\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "We managed to deploy our neuron compiled BERT to AWS Inferentia on Amazon SageMaker. Now, let's test its performance of it. As a dummy load test will we use threading to send 10000 requests to our endpoint with 10 threads.\n",
    "\n",
    "_Note: When running the load test we environment was based in europe and the endpoint is deployed in us-east-2._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "number_of_threads = 10\n",
    "number_of_requests = int(10000 // number_of_threads)\n",
    "print(f\"number of threads: {number_of_threads}\")\n",
    "print(f\"number of requests per thread: {number_of_requests}\")\n",
    "\n",
    "def send_rquests():\n",
    "    for _ in range(number_of_requests):\n",
    "        predictor.predict(data={\"inputs\": \"it 's a charming and often affecting journey .\"})\n",
    "    print(\"done\")\n",
    "\n",
    "# Create multiple threads\n",
    "threads = [threading.Thread(target=send_rquests) for _ in range(number_of_threads) ]\n",
    "# start all threads\n",
    "[t.start() for t in threads]\n",
    "# wait for all threads to finish\n",
    "[t.join() for t in threads]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0b8e25d",
   "metadata": {},
   "source": [
    "Sending 10000 requests with 10 threads takes around 86 seconds. This means we can run around ~116 inferences per second. But keep in mind that includes the network latency from europe to us-east-2. \n",
    "When we inspect the latency of the endpoint through cloudwatch we can see that the average latency is around 4ms. This means we can run around 500 inferences per second, without network overhead or framework overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"https://console.aws.amazon.com/cloudwatch/home?region={sess.boto_region_name}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'{predictor.endpoint_name}~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'{sess.boto_region_name}~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor.endpoint_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0f26d0",
   "metadata": {},
   "source": [
    "The average latency for our BERT model is `3.8-4.1ms` for a sequence length of 128.  \n",
    "\n",
    "![performance](./imgs/performance.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1030c87f",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8917d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656e81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
