{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e90d5c",
   "metadata": {},
   "source": [
    "# Optimized & deploy BERT on AWS inferentia2 with Amazon SageMaker\n",
    "\n",
    "In this end-to-end tutorial, you will learn how to speed up BERT inference down to `1ms` latency for text classification with Hugging Face Transformers, Amazon SageMaker, and AWS Inferentia2.\n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "1. Convert BERT to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "2. Create a custom `inference.py` script for `text-classification`\n",
    "3. Upload the neuron model and inference script to Amazon S3\n",
    "4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "5. Run and evaluate Inference performance of BERT on Inferentia2\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can findÂ [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)Â more about it.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3db68e5",
   "metadata": {},
   "source": [
    "## 1. Convert BERT to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "\n",
    "We are going to use the [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index). ðŸ¤— Optimum Neuron is the interface between the ðŸ¤— Transformers library and AWS Accelerators including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks. \n",
    "\n",
    "As a first step, we need to install the `optimum-neuron` and other required packages.\n",
    "\n",
    "*Tip: If you are using Amazon SageMaker Notebook Instances or Studio you can go with the `conda_python3` conda kernel.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: sagemaker 2.161.1.dev0\n",
      "Uninstalling sagemaker-2.161.1.dev0:\n",
      "  Successfully uninstalled sagemaker-2.161.1.dev0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1\n",
      "  Cloning https://github.com/philschmid/sagemaker-python-sdk (to revision patch-1) to /tmp/pip-install-fgbng3_i/sagemaker_8883e196a64941e09ec0374625cad0c7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/philschmid/sagemaker-python-sdk /tmp/pip-install-fgbng3_i/sagemaker_8883e196a64941e09ec0374625cad0c7\n",
      "  Running command git checkout -b patch-1 --track origin/patch-1\n",
      "  Switched to a new branch 'patch-1'\n",
      "  Branch 'patch-1' set up to track remote branch 'patch-1' from 'origin'.\n",
      "  Resolved https://github.com/philschmid/sagemaker-python-sdk to commit d77485cea67860641e4a3f143029413ed7f31f01\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.26.135)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.21.6)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (3.19.6)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.5.0)\n",
      "Requirement already satisfied: pathos in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==6.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (4.16.0)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (2.5.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.7.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.135 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.29.135)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (3.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from packaging>=20.0->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from jsonschema->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (2022.4)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.7.6.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from schema->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.135->boto3<2.0,>=1.26.131->sagemaker@ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1) (1.26.12)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.161.1.dev0-py2.py3-none-any.whl size=1078259 sha256=05253ce8621b56afa331c16883655933bc0332fac62aeece16fee5bba4e94ae9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7e5jxdr7/wheels/0f/14/98/d2bd916cc4293898ddfb987faea1d717a96756395c45e5e00c\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "Successfully installed sagemaker-2.161.1.dev0\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "# !pip install \"optimum-neuron[neuronx]==0.0.4\"  --upgrade\n",
    "# !python -m pip install \"optimum-neuron[neuronx]==0.0.4\" \"sagemaker==2.162.0\"  --upgrade\n",
    "# pip install sagemaker from github\n",
    "!pip uninstall sagemaker -y\n",
    "!pip install 'sagemaker @ git+https://github.com/philschmid/sagemaker-python-sdk@patch-1' --upgrade --no-cache-dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0ef431",
   "metadata": {},
   "source": [
    "After we have installed the `optimum-neuron` we can convert load and convert our model.\n",
    "\n",
    "We are going to use the [yiyanghkust/finbert-tone](https://huggingface.co/yiyanghkust/finbert-tone) model. FinBERT is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d8dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"yiyanghkust/finbert-tone\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "At the time of writing, the [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/dynamic-shapes.html?highlight=dynamic%20shapes#), which means that the input size needs to be static for compiling and inference. \n",
    "\n",
    "In simpler terms, this means when the model is converted with a sequence length of 16. The model can only run inference on inputs with the same shape.\n",
    "\n",
    "_When using a `t2.medium` instance the compiling takes around 2-3 minutes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcceb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Neuron: --auto-cast all\n",
      "Using Neuron: --auto-cast-type bf16\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from optimum.exporters.neuron import export\n",
    "from optimum.exporters.neuron.model_configs import DistilBertNeuronConfig\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "# define dummy input, sequence length and configuratio\n",
    "dummy_input = \"dummy input which will be padded later\"\n",
    "sequence_length = 128\n",
    "output_path = Path(\"tmp\")\n",
    "neuron_config = DistilBertNeuronConfig(\n",
    "    config=model.config, \n",
    "    task=\"text-classification\",\n",
    "    batch_size=1, \n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Export BERT to Neuron model (inferentia2)\n",
    "export(\n",
    "    model=model,\n",
    "    config=neuron_config,\n",
    "    output=output_path / \"model.neuron\",\n",
    "    auto_cast=\"all\",\n",
    "    auto_cast_type=\"bf16\",\n",
    ")\n",
    "# include sequence length in model config\n",
    "model.config.__setattr__(\"neuron_sequence_length\", sequence_length)\n",
    "# save tokenizer and model config\n",
    "tokenizer.save_pretrained(output_path)\n",
    "model.config.save_pretrained(output_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9997e9db",
   "metadata": {},
   "source": [
    "## 2. Create a custom `inference.py` script for `text-classification`\n",
    "\n",
    "The [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) supports zero-code deployments on top of theÂ [pipelineÂ feature](https://huggingface.co/transformers/main_classes/pipelines.html)Â from ðŸ¤— Transformers. This allows users to deploy Hugging Face transformers without an inference script [[Example](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)]. \n",
    "\n",
    "Currently is this feature not supported with AWS Inferentia2, which means we need to provide an `inference.py` for running inference. \n",
    "\n",
    "To use the inference script, we need to create anÂ `inference.py`Â script. In our example, we are going to overwrite theÂ `model_fn`Â to load our neuron model and theÂ `predict_fn`Â to create a sentence-embeddings pipeline. \n",
    "\n",
    "If you want to know more about the `inference.py`Â script check out this [example](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb). It explains amongst other things what the `model_fn` and `predict_fn` are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜codeâ€™: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce675df9",
   "metadata": {},
   "source": [
    "We are using the `NEURON_RT_NUM_CORES=1` to make sure that each HTTP worker uses 1 Neuron core to maximize throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "# To use one neuron core per worker\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"1\"\n",
    "AWS_NEURON_TRACED_WEIGHTS_NAME = \"model.neuron\"\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load tokenizer and neuron model from model_dir\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = torch.jit.load(os.path.join(model_dir, AWS_NEURON_TRACED_WEIGHTS_NAME))\n",
    "    model_config = AutoConfig.from_pretrained(model_dir)\n",
    "\n",
    "    return model, tokenizer, model_config\n",
    "\n",
    "\n",
    "def predict_fn(data, model_tokenizer_model_config):\n",
    "    # destruct model, tokenizer and model config\n",
    "    model, tokenizer, model_config = model_tokenizer_model_config\n",
    "\n",
    "    # create embeddings for inputs\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    embeddings = tokenizer(\n",
    "        inputs,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=model_config.neuron_sequence_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    # convert to tuple for neuron model\n",
    "    neuron_inputs = tuple(embeddings.values())\n",
    "\n",
    "    # run prediciton\n",
    "    with torch.no_grad():\n",
    "        predictions = model(*neuron_inputs)[0]\n",
    "        scores = torch.nn.Softmax(dim=1)(predictions)\n",
    "\n",
    "    # return dictonary, which will be json serializable\n",
    "    return [{\"label\": model_config.id2label[item.argmax().item()], \"score\": item.max().item()} for item in scores]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## 3. Upload the neuron model and inference script to Amazon S3\n",
    "\n",
    "Before we can deploy our neuron model to Amazon SageMaker we need to create a `model.tar.gz` archive with all our model artifacts saved into, e.g.Â `model.neuron` and upload this to Amazon S3.\n",
    "\n",
    "To do this we need to set up our permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952983b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ff630",
   "metadata": {},
   "source": [
    "Next, we create our `model.tar.gz`.TheÂ `inference.py`Â script will be placed into aÂ `code/`Â folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3808b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/tmp\n",
      "code/\n",
      "code/inference.py\n",
      "config.json\n",
      "model.neuron\n",
      "neuron.model\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "vocab.txt\n",
      "/home/ubuntu/huggingface-inferentia2-samples/bert-transformers\n"
     ]
    }
   ],
   "source": [
    "# copy inference.py into the code/ directory of the model directory.\n",
    "!cp -r code/ tmp/code/\n",
    "# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd tmp\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6f330",
   "metadata": {},
   "source": [
    "Now we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6146af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifcats uploaded to s3://sagemaker-us-east-1-558105141721/neuronx/yiyanghkust/finbert-tone/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/neuronx/{model_id}\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e1395a",
   "metadata": {},
   "source": [
    "## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "\n",
    "After we have uploaded ourÂ `model.tar.gz`Â to Amazon S3 can we create a customÂ `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2ae919",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_model_uri=\"s3://sagemaker-us-east-1-558105141721/neuronx/yiyanghkust/finbert-tone/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41522ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 4.28.1. Ignoring framework/algorithm version: 4.28.0.\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: Requested image 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.0-transformers4.28.0-neuronx-py38-ubuntu20.04 not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/sagemaker-notebook.ipynb Cell 20\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/sagemaker-notebook.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m huggingface_model\u001b[39m.\u001b[39m_is_compiled_model \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/sagemaker-notebook.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# deploy the endpoint endpoint\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/sagemaker-notebook.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m predictor \u001b[39m=\u001b[39m huggingface_model\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/sagemaker-notebook.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     initial_instance_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,      \u001b[39m# number of instances\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/sagemaker-notebook.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     instance_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mml.inf2.xlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m# AWS Inferentia Instance\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/sagemaker-notebook.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/huggingface/model.py:313\u001b[0m, in \u001b[0;36mHuggingFaceModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     inference_tool \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mneuron\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m instance_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mml.inf1\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mneuronx\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_uri \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserving_image_uri(\n\u001b[1;32m    308\u001b[0m         region_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mboto_session\u001b[39m.\u001b[39mregion_name,\n\u001b[1;32m    309\u001b[0m         instance_type\u001b[39m=\u001b[39minstance_type,\n\u001b[1;32m    310\u001b[0m         inference_tool\u001b[39m=\u001b[39minference_tool\n\u001b[1;32m    311\u001b[0m     )\n\u001b[0;32m--> 313\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(HuggingFaceModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m    314\u001b[0m     initial_instance_count,\n\u001b[1;32m    315\u001b[0m     instance_type,\n\u001b[1;32m    316\u001b[0m     serializer,\n\u001b[1;32m    317\u001b[0m     deserializer,\n\u001b[1;32m    318\u001b[0m     accelerator_type,\n\u001b[1;32m    319\u001b[0m     endpoint_name,\n\u001b[1;32m    320\u001b[0m     tags,\n\u001b[1;32m    321\u001b[0m     kms_key,\n\u001b[1;32m    322\u001b[0m     wait,\n\u001b[1;32m    323\u001b[0m     data_capture_config,\n\u001b[1;32m    324\u001b[0m     async_inference_config,\n\u001b[1;32m    325\u001b[0m     serverless_inference_config,\n\u001b[1;32m    326\u001b[0m     volume_size\u001b[39m=\u001b[39;49mvolume_size,\n\u001b[1;32m    327\u001b[0m     model_data_download_timeout\u001b[39m=\u001b[39;49mmodel_data_download_timeout,\n\u001b[1;32m    328\u001b[0m     container_startup_health_check_timeout\u001b[39m=\u001b[39;49mcontainer_startup_health_check_timeout,\n\u001b[1;32m    329\u001b[0m     inference_recommendation_id\u001b[39m=\u001b[39;49minference_recommendation_id,\n\u001b[1;32m    330\u001b[0m     explainer_config\u001b[39m=\u001b[39;49mexplainer_config,\n\u001b[1;32m    331\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/model.py:1278\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_name, compiled_model_suffix))\n\u001b[0;32m-> 1278\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_sagemaker_model(\n\u001b[1;32m   1279\u001b[0m     instance_type, accelerator_type, tags, serverless_inference_config\n\u001b[1;32m   1280\u001b[0m )\n\u001b[1;32m   1282\u001b[0m serverless_inference_config_dict \u001b[39m=\u001b[39m (\n\u001b[1;32m   1283\u001b[0m     serverless_inference_config\u001b[39m.\u001b[39m_to_request_dict() \u001b[39mif\u001b[39;00m is_serverless \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m )\n\u001b[1;32m   1285\u001b[0m production_variant \u001b[39m=\u001b[39m sagemaker\u001b[39m.\u001b[39mproduction_variant(\n\u001b[1;32m   1286\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m   1287\u001b[0m     instance_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     container_startup_health_check_timeout\u001b[39m=\u001b[39mcontainer_startup_health_check_timeout,\n\u001b[1;32m   1294\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/model.py:746\u001b[0m, in \u001b[0;36mModel._create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags, serverless_inference_config)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_network_isolation \u001b[39m=\u001b[39m resolve_value_from_config(\n\u001b[1;32m    734\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_network_isolation,\n\u001b[1;32m    735\u001b[0m     MODEL_ENABLE_NETWORK_ISOLATION_PATH,\n\u001b[1;32m    736\u001b[0m     sagemaker_session\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session,\n\u001b[1;32m    737\u001b[0m )\n\u001b[1;32m    738\u001b[0m create_model_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    739\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    740\u001b[0m     role\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrole,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    744\u001b[0m     tags\u001b[39m=\u001b[39mtags,\n\u001b[1;32m    745\u001b[0m )\n\u001b[0;32m--> 746\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mcreate_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcreate_model_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/session.py:3445\u001b[0m, in \u001b[0;36mSession.create_model\u001b[0;34m(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\u001b[0m\n\u001b[1;32m   3442\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3443\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 3445\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_intercept_create_request(create_model_request, submit, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_model\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   3446\u001b[0m \u001b[39mreturn\u001b[39;00m name\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/session.py:5306\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   5289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_intercept_create_request\u001b[39m(\n\u001b[1;32m   5290\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   5291\u001b[0m     request: typing\u001b[39m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5294\u001b[0m     \u001b[39m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   5295\u001b[0m ):\n\u001b[1;32m   5296\u001b[0m     \u001b[39m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   5297\u001b[0m \n\u001b[1;32m   5298\u001b[0m \u001b[39m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5304\u001b[0m \u001b[39m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   5305\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5306\u001b[0m     \u001b[39mreturn\u001b[39;00m create(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/session.py:3433\u001b[0m, in \u001b[0;36mSession.create_model.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   3431\u001b[0m LOGGER\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreateModel request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, json\u001b[39m.\u001b[39mdumps(request, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m))\n\u001b[1;32m   3432\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3433\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_client\u001b[39m.\u001b[39;49mcreate_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest)\n\u001b[1;32m   3434\u001b[0m \u001b[39mexcept\u001b[39;00m ClientError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3435\u001b[0m     error_code \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mresponse[\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: Requested image 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.0-transformers4.28.0-neuronx-py38-ubuntu20.04 not found."
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,      # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.28.0\",  # transformers version used\n",
    "   pytorch_version=\"1.13.0\",       # pytorch version used\n",
    "   py_version='py38',            # python version used\n",
    "   model_server_workers=2,       # number of workers for the model server\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.xlarge\" # AWS Inferentia Instance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a430e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=\"s3://mybucket/train\",\n",
    "    transformers_version=\"4.28\",\n",
    "    role=role,\n",
    "    pytorch_version=\"1.13\",\n",
    "    py_version=\"py38\",\n",
    ")\n",
    "container = huggingface_model.prepare_container_def(\"ml.inf2.xlarge\", inference_tool=\"neuronx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1291990d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13-transformers4.28-neuronx-py38-ubuntu20.04',\n",
       " 'Environment': {'SAGEMAKER_PROGRAM': '',\n",
       "  'SAGEMAKER_SUBMIT_DIRECTORY': '',\n",
       "  'SAGEMAKER_CONTAINER_LOG_LEVEL': '20',\n",
       "  'SAGEMAKER_REGION': 'us-east-1'},\n",
       " 'ModelDataUrl': 's3://mybucket/train'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.0-transformers4.28.0-neuronx-py38-ubuntu20.04\n",
    "# 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference-neuronx:1.13.0-transformers4.28.1-neuronx-py38-sdk2.9.1-ubuntu20.04\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26021331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 4.28.1. Ignoring framework/algorithm version: 4.28.0.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported base framework: pytorch1.13.1. You may need to upgrade your SDK version (pip install -U sagemaker) for newer base frameworks. Supported base framework(s): version_aliases, pytorch1.13.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/sagemaker-notebook.ipynb Cell 21\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bc6i/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/sagemaker-notebook.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m huggingface_model\u001b[39m.\u001b[39;49mprepare_container_def(\u001b[39m\"\u001b[39;49m\u001b[39mml.inf2.xlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m, inference_tool\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mneuronx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/huggingface/model.py:485\u001b[0m, in \u001b[0;36mHuggingFaceModel.prepare_container_def\u001b[0;34m(self, instance_type, accelerator_type, serverless_inference_config, inference_tool)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust supply either an instance type (for choosing CPU vs GPU) or an image URI.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         )\n\u001b[1;32m    484\u001b[0m     region_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mboto_session\u001b[39m.\u001b[39mregion_name\n\u001b[0;32m--> 485\u001b[0m     deploy_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserving_image_uri(\n\u001b[1;32m    486\u001b[0m         region_name,\n\u001b[1;32m    487\u001b[0m         instance_type,\n\u001b[1;32m    488\u001b[0m         accelerator_type\u001b[39m=\u001b[39;49maccelerator_type,\n\u001b[1;32m    489\u001b[0m         serverless_inference_config\u001b[39m=\u001b[39;49mserverless_inference_config,\n\u001b[1;32m    490\u001b[0m         inference_tool\u001b[39m=\u001b[39;49minference_tool,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[1;32m    493\u001b[0m deploy_key_prefix \u001b[39m=\u001b[39m model_code_key_prefix(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_prefix, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, deploy_image)\n\u001b[1;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upload_code(deploy_key_prefix, repack\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/huggingface/model.py:539\u001b[0m, in \u001b[0;36mHuggingFaceModel.serving_image_uri\u001b[0;34m(self, region_name, instance_type, accelerator_type, serverless_inference_config, inference_tool)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    538\u001b[0m     base_framework_version \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpytorch\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpytorch_version\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m  \u001b[39m# pylint: disable=no-member\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[39mreturn\u001b[39;00m image_uris\u001b[39m.\u001b[39;49mretrieve(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_framework_name,\n\u001b[1;32m    541\u001b[0m     region_name,\n\u001b[1;32m    542\u001b[0m     version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mframework_version,\n\u001b[1;32m    543\u001b[0m     py_version\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpy_version,\n\u001b[1;32m    544\u001b[0m     instance_type\u001b[39m=\u001b[39;49minstance_type,\n\u001b[1;32m    545\u001b[0m     accelerator_type\u001b[39m=\u001b[39;49maccelerator_type,\n\u001b[1;32m    546\u001b[0m     image_scope\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minference\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    547\u001b[0m     base_framework_version\u001b[39m=\u001b[39;49mbase_framework_version,\n\u001b[1;32m    548\u001b[0m     serverless_inference_config\u001b[39m=\u001b[39;49mserverless_inference_config,\n\u001b[1;32m    549\u001b[0m     inference_tool\u001b[39m=\u001b[39;49minference_tool,\n\u001b[1;32m    550\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/workflow/utilities.py:388\u001b[0m, in \u001b[0;36moverride_pipeline_parameter_var.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         logger\u001b[39m.\u001b[39mwarning(warning_msg_template, arg_name, func_name, \u001b[39mtype\u001b[39m(value))\n\u001b[1;32m    387\u001b[0m         kwargs[arg_name] \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mdefault_value\n\u001b[0;32m--> 388\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/image_uris.py:176\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(framework, region, version, py_version, instance_type, accelerator_type, image_scope, container_version, distribution, base_framework_version, training_compiler_config, model_id, model_version, tolerate_vulnerable_model, tolerate_deprecated_model, sdk_version, inference_tool, serverless_inference_config)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;00m version_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mversion_aliases\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    173\u001b[0m         full_base_framework_version \u001b[39m=\u001b[39m version_config[\u001b[39m\"\u001b[39m\u001b[39mversion_aliases\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\n\u001b[1;32m    174\u001b[0m             base_framework_version, base_framework_version\n\u001b[1;32m    175\u001b[0m         )\n\u001b[0;32m--> 176\u001b[0m     _validate_arg(full_base_framework_version, \u001b[39mlist\u001b[39;49m(version_config\u001b[39m.\u001b[39;49mkeys()), \u001b[39m\"\u001b[39;49m\u001b[39mbase framework\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    177\u001b[0m     version_config \u001b[39m=\u001b[39m version_config\u001b[39m.\u001b[39mget(full_base_framework_version)\n\u001b[1;32m    179\u001b[0m py_version \u001b[39m=\u001b[39m _validate_py_version_and_set_if_needed(py_version, version_config, framework)\n",
      "File \u001b[0;32m~/miniconda3/envs/dev/lib/python3.9/site-packages/sagemaker/image_uris.py:581\u001b[0m, in \u001b[0;36m_validate_arg\u001b[0;34m(arg, available_options, arg_name)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39m\"\"\"Checks if the arg is in the available options, and raises a ``ValueError`` if not.\"\"\"\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m arg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m available_options:\n\u001b[0;32m--> 581\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    582\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnsupported \u001b[39m\u001b[39m{arg_name}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{arg}\u001b[39;00m\u001b[39m. You may need to upgrade your SDK version \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(pip install -U sagemaker) for newer \u001b[39m\u001b[39m{arg_name}\u001b[39;00m\u001b[39ms. Supported \u001b[39m\u001b[39m{arg_name}\u001b[39;00m\u001b[39m(s): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    584\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m{options}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(arg_name\u001b[39m=\u001b[39marg_name, arg\u001b[39m=\u001b[39marg, options\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(available_options))\n\u001b[1;32m    585\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported base framework: pytorch1.13.1. You may need to upgrade your SDK version (pip install -U sagemaker) for newer base frameworks. Supported base framework(s): version_aliases, pytorch1.13.0."
     ]
    }
   ],
   "source": [
    "huggingface_model.prepare_container_def(\"ml.inf2.xlarge\", inference_tool=\"neuronx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c858560",
   "metadata": {},
   "source": [
    "# 5. Run and evaluate Inference performance of BERT on Inferentia\n",
    "\n",
    "TheÂ `.deploy()`Â returns anÂ `HuggingFacePredictor`Â object which can be used to request inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ff049",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"inputs\": \"the mesmerizing performances of the leads keep the film grounded and keep the audience riveted .\",\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "We managed to deploy our neuron compiled BERT to AWS Inferentia on Amazon SageMaker. Now, let's test its performance of it. As a dummy load test will we loop and send 10000 synchronous requests to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dcfd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send 10000 requests\n",
    "for i in range(10000):\n",
    "    resp = predictor.predict(\n",
    "        data={\"inputs\": \"it 's a charming and often affecting journey .\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3812f",
   "metadata": {},
   "source": [
    "Let's inspect the performance in cloudwatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"https://console.aws.amazon.com/cloudwatch/home?region={sess.boto_region_name}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'{predictor.endpoint_name}~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'{sess.boto_region_name}~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor.endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f26d0",
   "metadata": {},
   "source": [
    "The average latency for our MiniLM model is `3-4.5ms` for a sequence length of 128.  \n",
    "\n",
    "![performance](./imgs/performance.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1030c87f",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8917d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
