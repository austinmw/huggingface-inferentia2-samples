{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e90d5c",
   "metadata": {},
   "source": [
    "# Optimized & deploy BERT on AWS inferentia2 with Amazon SageMaker\n",
    "\n",
    "In this end-to-end tutorial, you will learn how to speed up BERT inference down to `1ms` latency for text classification with Hugging Face Transformers, Amazon SageMaker, and AWS Inferentia2.\n",
    "\n",
    "You will learn how to: \n",
    "\n",
    "1. Convert BERT to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "2. Create a custom `inference.py` script for `text-classification`\n",
    "3. Upload the neuron model and inference script to Amazon S3\n",
    "4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "5. Run and evaluate Inference performance of BERT on Inferentia2\n",
    "\n",
    "Let's get started! ðŸš€\n",
    "\n",
    "---\n",
    "\n",
    "*If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can findÂ [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)Â more about it.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3db68e5",
   "metadata": {},
   "source": [
    "## 1. Convert BERT to AWS Neuron (Inferentia2) with `optimum-neuron`\n",
    "\n",
    "We are going to use the [optimum-neuron](https://huggingface.co/docs/optimum-neuron/index). ðŸ¤— Optimum Neuron is the interface between the ðŸ¤— Transformers library and AWS Accelerators including [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/?nc1=h_ls) and [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/?nc1=h_ls). It provides a set of tools enabling easy model loading, training and inference on single- and multi-Accelerator settings for different downstream tasks. \n",
    "\n",
    "As a first step, we need to install the `optimum-neuron` and other required packages.\n",
    "\n",
    "*Tip: If you are using Amazon SageMaker Notebook Instances or Studio you can go with the `conda_python3` conda kernel.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c59d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting git+https://github.com/huggingface/optimum-neuron.git@add-pipelines\n",
      "  Cloning https://github.com/huggingface/optimum-neuron.git (to revision add-pipelines) to /tmp/pip-req-build-pdhfqa2n\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/optimum-neuron.git /tmp/pip-req-build-pdhfqa2n\n",
      "  Running command git checkout -b add-pipelines --track origin/add-pipelines\n",
      "  Switched to a new branch 'add-pipelines'\n",
      "  Branch 'add-pipelines' set up to track remote branch 'add-pipelines' from 'origin'.\n",
      "  Resolved https://github.com/huggingface/optimum-neuron.git to commit 39b7cee312c1184341e523d49754d26022d484c4\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron==0.0.6.dev0) (0.15.1)\n",
      "Requirement already satisfied: optimum in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron==0.0.6.dev0) (1.8.6)\n",
      "Requirement already satisfied: accelerate>=0.20.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron==0.0.6.dev0) (0.20.3)\n",
      "Requirement already satisfied: numpy<=1.21.6,>=1.18.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron==0.0.6.dev0) (1.21.6)\n",
      "Requirement already satisfied: protobuf<4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron==0.0.6.dev0) (3.19.6)\n",
      "Requirement already satisfied: transformers>=4.28.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum-neuron==0.0.6.dev0) (4.29.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (1.13.1)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (5.9.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (21.3)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (6.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->optimum-neuron==0.0.6.dev0) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->optimum-neuron==0.0.6.dev0) (4.64.1)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->optimum-neuron==0.0.6.dev0) (2022.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->optimum-neuron==0.0.6.dev0) (4.4.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->optimum-neuron==0.0.6.dev0) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers>=4.28.0->optimum-neuron==0.0.6.dev0) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers>=4.28.0->optimum-neuron==0.0.6.dev0) (0.13.1)\n",
      "Requirement already satisfied: datasets in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum->optimum-neuron==0.0.6.dev0) (2.9.0)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum->optimum-neuron==0.0.6.dev0) (0.14.1)\n",
      "Requirement already satisfied: coloredlogs in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum->optimum-neuron==0.0.6.dev0) (15.0.1)\n",
      "Requirement already satisfied: sympy in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from optimum->optimum-neuron==0.0.6.dev0) (1.11.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from packaging>=20.0->accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (3.0.9)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (67.7.2)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate>=0.20.1->optimum-neuron==0.0.6.dev0) (0.37.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from transformers>=4.28.0->optimum-neuron==0.0.6.dev0) (0.1.97)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from coloredlogs->optimum->optimum-neuron==0.0.6.dev0) (10.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum->optimum-neuron==0.0.6.dev0) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum->optimum-neuron==0.0.6.dev0) (0.3.6)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum->optimum-neuron==0.0.6.dev0) (9.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum->optimum-neuron==0.0.6.dev0) (0.70.14)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum->optimum-neuron==0.0.6.dev0) (1.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum->optimum-neuron==0.0.6.dev0) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from datasets->optimum->optimum-neuron==0.0.6.dev0) (3.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests->huggingface-hub>=0.14.0->optimum-neuron==0.0.6.dev0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests->huggingface-hub>=0.14.0->optimum-neuron==0.0.6.dev0) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests->huggingface-hub>=0.14.0->optimum-neuron==0.0.6.dev0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from requests->huggingface-hub>=0.14.0->optimum-neuron==0.0.6.dev0) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sympy->optimum->optimum-neuron==0.0.6.dev0) (1.2.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from torchvision->optimum->optimum-neuron==0.0.6.dev0) (9.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.6.dev0) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.6.dev0) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.6.dev0) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.6.dev0) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.6.dev0) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from aiohttp->datasets->optimum->optimum-neuron==0.0.6.dev0) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->datasets->optimum->optimum-neuron==0.0.6.dev0) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->datasets->optimum->optimum-neuron==0.0.6.dev0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum->optimum-neuron==0.0.6.dev0) (1.16.0)\n",
      "Building wheels for collected packages: optimum-neuron\n",
      "  Building wheel for optimum-neuron (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum-neuron: filename=optimum_neuron-0.0.6.dev0-py3-none-any.whl size=118823 sha256=1eba9ecfd1352b7e72127b472af58ebd95f97ee5e28794380d469b85733837bb\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-teu1f14m/wheels/94/56/48/f8de85748e0f15d1d98995a3d0491a0ba884dfdebdf2120c73\n",
      "Successfully built optimum-neuron\n",
      "Installing collected packages: optimum-neuron\n",
      "  Attempting uninstall: optimum-neuron\n",
      "    Found existing installation: optimum-neuron 0.0.5\n",
      "    Uninstalling optimum-neuron-0.0.5:\n",
      "      Successfully uninstalled optimum-neuron-0.0.5\n",
      "Successfully installed optimum-neuron-0.0.6.dev0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting git+https://github.com/aws/sagemaker-python-sdk.git\n",
      "  Cloning https://github.com/aws/sagemaker-python-sdk.git to /tmp/pip-req-build-z3y5si34\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/aws/sagemaker-python-sdk.git /tmp/pip-req-build-z3y5si34\n",
      "  Resolved https://github.com/aws/sagemaker-python-sdk.git to commit c1b24651517d4dd4d54f62c9565bdee9012fd1e2\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (1.26.135)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (1.21.6)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (3.19.6)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (1.5.0)\n",
      "Requirement already satisfied: pathos in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (0.7.5)\n",
      "Requirement already satisfied: PyYAML==6.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (4.16.0)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (2.5.2)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from sagemaker==2.168.1.dev0) (1.7.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.168.1.dev0) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.135 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.168.1.dev0) (1.29.135)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from boto3<2.0,>=1.26.131->sagemaker==2.168.1.dev0) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker==2.168.1.dev0) (3.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from packaging>=20.0->sagemaker==2.168.1.dev0) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker==2.168.1.dev0) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from jsonschema->sagemaker==2.168.1.dev0) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->sagemaker==2.168.1.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pandas->sagemaker==2.168.1.dev0) (2022.4)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker==2.168.1.dev0) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker==2.168.1.dev0) (0.3.2)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker==2.168.1.dev0) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from pathos->sagemaker==2.168.1.dev0) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from schema->sagemaker==2.168.1.dev0) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ubuntu/miniconda3/envs/dev/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.135->boto3<2.0,>=1.26.131->sagemaker==2.168.1.dev0) (1.26.12)\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "# !pip install \"optimum-neuron[neuronx]==0.0.6\"  --upgrade\n",
    "!pip install \"git+https://github.com/huggingface/optimum-neuron.git@add-pipelines\"  --upgrade\n",
    "\n",
    "# !python -m pip install \"sagemaker==2.169.0\"  --upgrade\n",
    "!python -m pip install \"git+https://github.com/aws/sagemaker-python-sdk.git\"  --upgrade\n",
    "# pip install sagemaker from github"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0ef431",
   "metadata": {},
   "source": [
    "After we have installed the `optimum-neuron` we can convert load and convert our model.\n",
    "\n",
    "We are going to use the [yiyanghkust/finbert-tone](https://huggingface.co/yiyanghkust/finbert-tone) model. FinBERT is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens. This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d8dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"yiyanghkust/finbert-tone\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e4386d9",
   "metadata": {},
   "source": [
    "At the time of writing, the [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/dynamic-shapes.html?highlight=dynamic%20shapes#), which means that the input size needs to be static for compiling and inference. \n",
    "\n",
    "In simpler terms, this means when the model is converted with a sequence length of 16. The model can only run inference on inputs with the same shape. We are going to use the `optimum-cli` to convert our model with a sequence length of 128 and a batch size of 1. \n",
    "\n",
    "_When using a `t2.medium` instance the compiling takes around 2-3 minutes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44ff097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID: yiyanghkust/finbert-tone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 14:29:24.811536: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-27 14:29:24.811595: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-27 14:29:24.811602: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-Jun-27 14:29:27.0294  4641:4641  ERROR  TDRV:tdrv_get_dev_info                       No neuron device available\n",
      "2023-06-27 14:29:31.373076: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-27 14:29:31.373125: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-27 14:29:31.373129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Validating Neuron model...\n",
      "2023-Jun-27 14:30:12.0849  4655:4655  ERROR  TDRV:tdrv_get_dev_info                       No neuron device available\n",
      "2023-Jun-27 14:30:12.0849  4655:4655  ERROR   NRT:nrt_init                                Cannot find Neuron devices. Please run on an instance type that supports Neuron, such as inf1 or trn1.\n",
      "2023-Jun-27 14:30:12.0849  4655:4655  ERROR   NRT:nrt_init                                If already on a supported instance type, make sure aws-neuronx-dkms is installed on the host and the neuron driver is accessible by referring to the troubleshooting guide: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/nrt-troubleshoot.html#neuron-driver-installation-fails.\n",
      "2023-Jun-27 14:30:12.0849  4655:4655  ERROR   NRT:nrt_init                                If you are using Neuron Containers, make sure the neuron devices are exposed to the container by referring to the container troubleshooting guide: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/containers/troubleshooting.html#neuron-device-not-found\n",
      "An error occured with the error message: The PyTorch Neuron Runtime could not be initialized. Neuron Driver issues are logged\n",
      "to your system logs. See the Neuron Runtime's troubleshooting guide for help on this\n",
      "topic: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/.\n",
      " The exported model was saved at: tmp\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$model_id\"\n",
    "MODEL_ID=$1\n",
    "SEQUENCE_LENGTH=128\n",
    "BATCH_SIZE=1\n",
    "OUTPUT_DIR=tmp/ # used to store temproary files\n",
    "echo \"Model ID: $MODEL_ID\"\n",
    "\n",
    "# exporting model\n",
    "optimum-cli export neuron \\\n",
    "  --model $MODEL_ID \\\n",
    "  --sequence_length $SEQUENCE_LENGTH \\\n",
    "  --batch_size $BATCH_SIZE \\\n",
    "  $OUTPUT_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9997e9db",
   "metadata": {},
   "source": [
    "## 2. Create a custom `inference.py` script for `text-classification`\n",
    "\n",
    "The [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) supports zero-code deployments on top of theÂ [pipelineÂ feature](https://huggingface.co/transformers/main_classes/pipelines.html)Â from ðŸ¤— Transformers. This allows users to deploy Hugging Face transformers without an inference script [[Example](https://github.com/huggingface/notebooks/blob/master/sagemaker/11_deploy_model_from_hf_hub/deploy_transformer_model_from_hf_hub.ipynb)]. \n",
    "\n",
    "Currently is this feature not supported with AWS Inferentia2, which means we need to provide an `inference.py` for running inference. But `optimum-neuron` has integrated support for the ðŸ¤— Transformers pipeline feature. That way we can use the `optimum-neuron` to create a pipeline for our model.\n",
    "\n",
    "If you want to know more about the `inference.py`Â script check out this [example](https://github.com/huggingface/notebooks/blob/master/sagemaker/17_custom_inference_script/sagemaker-notebook.ipynb). It explains amongst other things what the `model_fn` and `predict_fn` are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4246c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39f74625",
   "metadata": {},
   "source": [
    "In addition to our `inference.py` script we need to provide a `requirements.txt`, which installs the latest version of the `optimum-neuron` package, which comes with `pipeline` support for AWS Inferentia2. \n",
    "_Note: This is a temporary solution until the `optimum-neuron` package is updated inside the DLC._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09fd8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/requirements.txt\n",
    "git+https://github.com/huggingface/optimum-neuron.git@add-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce675df9",
   "metadata": {},
   "source": [
    "We are using the `NEURON_RT_NUM_CORES=1` to make sure that each HTTP worker uses 1 Neuron core to maximize throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce41529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import os\n",
    "# To use one neuron core per worker\n",
    "os.environ[\"NEURON_RT_NUM_CORES\"] = \"1\"\n",
    "from optimum.neuron.pipelines import pipeline\n",
    "import torch\n",
    "import torch_neuronx\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # load local converted model into pipeline\n",
    "    pipe = pipeline(\"text-classification\", model=model_dir)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_fn(data, pipeline):\n",
    "    inputs = data.pop(\"inputs\", data)\n",
    "    parameters = data.pop(\"parameters\", None)\n",
    "\n",
    "    # pass inputs with all kwargs in data\n",
    "    if parameters is not None:\n",
    "        prediction = pipeline(inputs, **parameters)\n",
    "    else:\n",
    "        prediction = pipeline(inputs)\n",
    "    # postprocess the prediction\n",
    "    return prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "144d8ccb",
   "metadata": {},
   "source": [
    "## 3. Upload the neuron model and inference script to Amazon S3\n",
    "\n",
    "Before we can deploy our neuron model to Amazon SageMaker we need to create a `model.tar.gz` archive with all our model artifacts saved into, e.g.Â `model.neuron` and upload this to Amazon S3.\n",
    "\n",
    "To do this we need to set up our permissions. Currently `inf2` instances are only available in the `us-east-2` region [[REF](https://aws.amazon.com/de/about-aws/whats-new/2023/05/sagemaker-ml-inf2-ml-trn1-instances-model-deployment/)]. Therefore we need to force the region to us-east-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d016feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-2\" # need to set to ohio region"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83ea3df1",
   "metadata": {},
   "source": [
    "Now lets create our SageMaker session and upload our model to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "952983b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philippschmid to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-2-558105141721\n",
      "sagemaker session region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "assert sess.boto_region_name == \"us-east-2\", \"region must be us-east-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ff630",
   "metadata": {},
   "source": [
    "Next, we create our `model.tar.gz`.TheÂ `inference.py`Â script will be placed into aÂ `code/`Â folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3808b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/huggingface-inferentia2-samples/bert-transformers/tmp\n",
      "code/\n",
      "code/requirements.txt\n",
      "code/inference.py\n",
      "config.json\n",
      "model.neuron\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "vocab.txt\n",
      "/home/ubuntu/huggingface-inferentia2-samples/bert-transformers\n"
     ]
    }
   ],
   "source": [
    "# copy inference.py into the code/ directory of the model directory.\n",
    "!cp -r code/ tmp/code/\n",
    "# create a model.tar.gz archive with all the model artifacts and the inference.py script.\n",
    "%cd tmp\n",
    "!tar zcvf model.tar.gz *\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a6f330",
   "metadata": {},
   "source": [
    "Now we can upload our `model.tar.gz` to our session S3 bucket with `sagemaker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6146af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifcats uploaded to s3://sagemaker-us-east-2-558105141721/neuronx/yiyanghkust/finbert-tone/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# create s3 uri\n",
    "s3_model_path = f\"s3://{sess.default_bucket()}/neuronx/{model_id}\"\n",
    "\n",
    "# upload model.tar.gz\n",
    "s3_model_uri = S3Uploader.upload(local_path=\"tmp/model.tar.gz\",desired_s3_uri=s3_model_path)\n",
    "print(f\"model artifcats uploaded to {s3_model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cccbb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tmp directory after uploading\n",
    "# !rm -rf tmp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04e1395a",
   "metadata": {},
   "source": [
    "## 4. Deploy a Real-time Inference Endpoint on Amazon SageMaker\n",
    "\n",
    "After we have uploaded ourÂ `model.tar.gz`Â to Amazon S3 can we create a customÂ `HuggingfaceModel`. This class will be used to create and deploy our real-time inference endpoint on Amazon SageMaker.\n",
    "\n",
    "The `inf2.xlarge` instance type is the smallest instance type with AWS Inferentia2 support. It comes with 1 Inferentia2 chip with 2 Neuron Cores. This means we can use 2 Model server workers to maximize throughput and run 2 inferences in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41522ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=s3_model_uri,        # path to your model and script\n",
    "   role=role,                      # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.28.1\",  # transformers version used\n",
    "   pytorch_version=\"1.13.0\",       # pytorch version used\n",
    "   py_version='py38',              # python version used\n",
    "   model_server_workers=2,         # number of workers for the model server\n",
    ")\n",
    "\n",
    "# Let SageMaker know that we've already compiled the model\n",
    "huggingface_model._is_compiled_model = True\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,      # number of instances\n",
    "    instance_type=\"ml.inf2.xlarge\" # AWS Inferentia Instance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c858560",
   "metadata": {},
   "source": [
    "# 5. Run and evaluate Inference performance of BERT on Inferentia\n",
    "\n",
    "TheÂ `.deploy()`Â returns anÂ `HuggingFacePredictor`Â object which can be used to request inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da2ff049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Neutral', 'score': 0.9999575614929199}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "  \"inputs\": \"the mesmerizing performances of the leads keep the film grounded and keep the audience riveted .\",\n",
    "}\n",
    "\n",
    "res = predictor.predict(data=data)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a146346",
   "metadata": {},
   "source": [
    "We managed to deploy our neuron compiled BERT to AWS Inferentia on Amazon SageMaker. Now, let's test its performance of it. As a dummy load test will we use threading to send 10000 requests to our endpoint with 10 threads.\n",
    "\n",
    "_Note: When running the load test we environment was based in europe and the endpoint is deployed in us-east-2._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "237f198c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of threads: 10\n",
      "number of requests per thread: 1000\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "number_of_threads = 10\n",
    "number_of_requests = int(10000 // number_of_threads)\n",
    "print(f\"number of threads: {number_of_threads}\")\n",
    "print(f\"number of requests per thread: {number_of_requests}\")\n",
    "\n",
    "def send_rquests():\n",
    "    for _ in range(number_of_requests):\n",
    "        predictor.predict(data={\"inputs\": \"it 's a charming and often affecting journey .\"})\n",
    "    print(\"done\")\n",
    "\n",
    "# Create multiple threads\n",
    "threads = [threading.Thread(target=send_rquests) for _ in range(number_of_threads) ]\n",
    "# start all threads\n",
    "[t.start() for t in threads]\n",
    "# wait for all threads to finish\n",
    "[t.join() for t in threads]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0b8e25d",
   "metadata": {},
   "source": [
    "Sending 10000 requests with 10 threads takes around 86 seconds. This means we can run around ~116 inferences per second. But keep in mind that includes the network latency from europe to us-east-2. \n",
    "When we inspect the latency of the endpoint through cloudwatch we can see that the average latency is around 4ms. This means we can run around 30000 inferences per second, without network overhead or framework overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a4d916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://console.aws.amazon.com/cloudwatch/home?region=us-east-2#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'huggingface-pytorch-inference-neuronx-m-2023-06-27-14-30-36-689~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'us-east-2~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20huggingface-pytorch-inference-neuronx-m-2023-06-27-14-30-36-689\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://console.aws.amazon.com/cloudwatch/home?region={sess.boto_region_name}#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'ModelLatency~'EndpointName~'{predictor.endpoint_name}~'VariantName~'AllTraffic))~view~'timeSeries~stacked~false~region~'{sess.boto_region_name}~start~'-PT5M~end~'P0D~stat~'Average~period~30);query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20{predictor.endpoint_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0f26d0",
   "metadata": {},
   "source": [
    "The average latency for our BERT model is `3.8-4.1ms` for a sequence length of 128.  \n",
    "\n",
    "![performance](./imgs/performance.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1030c87f",
   "metadata": {},
   "source": [
    "### Delete model and endpoint\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8917d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656e81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
