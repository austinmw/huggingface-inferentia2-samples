{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Mixtral 8x7B on AWS Inferentia2 with Hugging Face Optimum\n",
    "\n",
    "\n",
    "[Mixtral 8x7B](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) is the open LLM from [Mistral AI](https://huggingface.co/mistralai). The Mixtral-8x7B is a Sparse Mixture of Experts it has a similar architecture to Mistral 7B, but comes with a twist: it’s actually 8 “expert” models in one. If you want to learn more about MoEs check out [Mixture of Experts Explained](https://huggingface.co/blog/moe).\n",
    "\n",
    "In this blog you will learn how to deploy [NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) model on AWS Inferentia2 with Hugging Face Optimum on Amazon SageMaker. We are going to use the Hugging Face LLM Inf2 Container a new purpose-built Inference Container to easily deploy LLMs on AWS Inferentia2 powered by[ Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index) and [Optimum Neuron](https://huggingface.co/docs/optimum-neuron/index).\n",
    "\n",
    "\n",
    "In the blog will cover how to:\n",
    "1. [Setup development environment](#1-setup-development-environment)\n",
    "2. [Retrieve the new Hugging Face LLM Inf2 DLC](#2-retrieve-the-new-hugging-face-llm-inf2-dlc)\n",
    "3. [Deploy Mixtral 8x7B to inferentia2](#3-deploy-mixtral-8x7b-to-inferentia2)\n",
    "4. [Run inference and chat with the model](#4-run-inference-and-chat-with-the-model)\n",
    "5. [Benchmark Mixtral 8x7B with llmperf on AWS Inferentia2](#5-benchmark-mixtral-8x7b-with-llmperf-on-aws-inferentia2)\n",
    "6. [Clean up](#6-clean-up)\n",
    "\n",
    "Lets get started! 🚀\n",
    "\n",
    "## Quick intro: AWS Inferentia 2\n",
    "\n",
    "[AWS inferentia (Inf2)](https://aws.amazon.com/de/ec2/instance-types/inf2/) are purpose-built EC2 for deep learning (DL) inference workloads. Inferentia 2 is the successor of [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf1/?nc1=h_ls), which promises to deliver up to 4x higher throughput and up to 10x lower latency.\n",
    "\n",
    "| instance size | accelerators | Neuron Cores | accelerator memory | vCPU | CPU Memory | on-demand price ($/h) |\n",
    "| ------------- | ------------ | ------------ | ------------------ | ---- | ---------- | --------------------- |\n",
    "| inf2.xlarge   | 1            | 2            | 32                 | 4    | 16         | 0.76                  |\n",
    "| inf2.8xlarge  | 1            | 2            | 32                 | 32   | 128        | 1.97                  |\n",
    "| inf2.24xlarge | 6            | 12           | 192                | 96   | 384        | 6.49                  |\n",
    "| inf2.48xlarge | 12           | 24           | 384                | 192  | 768        | 12.98                 |\n",
    "\n",
    "Additionally, inferentia 2 will support the writing of custom operators in c++ and new datatypes, including `FP8` (cFP8).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy Mixtral to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.223.0\" \"gradio<4\" transformers --upgrade --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve the new Hugging Face LLM Inf2 DLC\n",
    "\n",
    "The new Hugging Face TGI Neuronx DLCs can be used to run inference on AWS Inferentia2. You can use the `get_huggingface_llm_image_uri` method of the `sagemaker` SDK to retrieve the appropriate Hugging Face TGI Neuronx DLC URI based on your desired `backend`, `session`, `region`, and `version`. You can find all the available versions [here](https://github.com/aws/deep-learning-containers/releases?q=tgi+AND+neuronx&expanded=true).\n",
    "\n",
    "*Note: At the time of writing this blog post the latest version of the Hugging Face LLM DLC is not yet available via the `get_huggingface_llm_image_uri` method. We are going to use the raw container uri instead.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface-neuronx\",\n",
    "  version=\"0.0.23\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy Mixtral 8x7B to inferentia2\n",
    "\n",
    "At the time of writing, [AWS Inferentia2 does not support dynamic shapes for inference](https://awsdocs-neuron.readthedocs-hosted.com/en/v2.6.0/general/arch/neuron-features/dynamic-shapes.html#neuron-dynamic-shapes), which means that we need to specify our sequence length and batch size ahead of time.\n",
    "\n",
    "Below is an example on how to compile Mixtral 8x7B with Optimum CLI, thats not needed in this case as we pre-compiled the model [aws-neuron/mixtral-instruct-seqlen-4096-bs-4-optimum-0-0-23](https://huggingface.co/aws-neuron/mixtral-instruct-seqlen-4096-bs-4-optimum-0-0-23) with a batch size of 4 and a sequence length of 4096.\n",
    "\n",
    "\n",
    "**Example: Compile Mixtral 8x7B with Optimum CLI**\n",
    "\n",
    "\n",
    "_Note: You need to compile models on an AWS EC2 instance with Inferentia2 support. Compilation can take up to 45 minutes if there is no cached configuration available._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
    "SEQUENCE_LENGTH = 4096\n",
    "BATCH_SIZE = 4\n",
    "NUM_CORES = 24 # each inferentia chip has 2 cores, e.g. inf2.48xlarge has 12 chips or 24 cores\n",
    "PRECISION = \"fp16\"\n",
    "HF_MODEL_ID_TO_PUSH=\"aws-neuron/hermes-mixtral-instruct-seqlen-4096-bs-4-optimum-0-0-23\" # change this to your desired model id\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# login into the huggingface hub to access gated models, like llama\n",
    "!huggingface-cli login --token $HF_TOKEN\n",
    "# compile model with optimum for batch size 4 and sequence length 2048\n",
    "!optimum-cli export neuron -m {MODEL_ID} --batch_size {BATCH_SIZE} --sequence_length {SEQUENCE_LENGTH} --num_cores {NUM_CORES} --auto_cast_type {PRECISION} ./mixtral-instruct-neuron\n",
    "# push model to hub [repo_id] [local_path] [path_in_repo]\n",
    "!huggingface-cli upload {HF_MODEL_ID_TO_PUSH} ./mixtral-instruct-neuron ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: We only compile and push the architecture and not the weights. Those will still be loaded from the original repository (NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO). If you also want to push the weights remove ` --exclude \"checkpoint/**\"` from the `upload` command. This has been avoided to speed up things._\n",
    "\n",
    "**Deploying Mixtral 8x7B as Endpoint**  \n",
    "\n",
    "Before deploying the model to Amazon SageMaker, we must define the TGI Neuronx endpoint configuration. We need to make sure the following additional parameters are defined: \n",
    "\n",
    "- `HF_MODEL_ID`: The Hugging Face model ID or path to where model is stored, e.g. `/opt/ml/model`.\n",
    "- `HF_NUM_CORES`: Number of Neuron Cores used for the compilation.\n",
    "- `MAX_BATCH_SIZE`: The maximum batch size that the model can handle, equal to the batch size used for compilation.\n",
    "- `MAX_INPUT_LENGTH`: The maximum input length that the model can handle, equal to the sequence length used for compilation.\n",
    "- `MAX_TOTAL_TOKENS`: The maximum total tokens the model can generate, equal to the sequence length used for compilation.\n",
    "- `HF_AUTO_CAST_TYPE`: The auto cast type that was used to compile the model.\n",
    "- `HF_TOKEN`: The Hugging Face API token to access gated models, optional if the model is public.\n",
    "\n",
    "**Select the right instance type**\n",
    "\n",
    "Mixtral 8x7B is a large model and requires a lot of memory. We are going to use the `inf2.48xlarge` instance type, which has 192 vCPUs and 384 GB of accelerator memory. The `inf2.48xlarge` instance comes with 12 Inferentia2 accelerators that include 24 Neuron Cores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.inf2.48xlarge\"\n",
    "health_check_timeout=2400 # additional time to load the model\n",
    "volume_size=512 # size in GB of the EBS volume\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "    \"HF_MODEL_ID\": \"aws-neuron/hermes-mixtral-instruct-seqlen-4096-bs-4-optimum-0-0-23\", # replace with your model id if you are using your own model\n",
    "    \"HF_NUM_CORES\": \"24\", # number of neuron cores\n",
    "    \"HF_AUTO_CAST_TYPE\": \"fp16\",  # dtype of the model\n",
    "    \"MAX_BATCH_SIZE\": \"4\", # max batch size for the model\n",
    "    \"MAX_INPUT_LENGTH\": \"4000\", # max length of input text\n",
    "    \"MAX_TOTAL_TOKENS\": \"4096\", # max length of generated text\n",
    "    \"MESSAGES_API_ENABLED\": \"true\", # Enable the messages API\n",
    "    # \"HF_TOKEN\": HfFolder.get_token(), # pass the huggingface token to access the base weights\n",
    "}\n",
    "\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.inf2.48xlarge` instance type. TGI will automatically distribute and shard the model across all Inferentia devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm_model._is_compiled_model = True # We precompiled the model\n",
    "\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout,\n",
    "  volume_size=volume_size\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes, we are working on improving the deployment time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run inference and chat with the model\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. We will use the `predict` method from the `predictor` to run inference on our endpoint. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [here](https://huggingface.co/docs/text-generation-inference/messages_api).\n",
    "\n",
    "The Messages API allows us to interact with the model in a conversational way. We can define the role of the message and the content. The role can be either `system`,`assistant` or `user`. The `system` role is used to provide context to the model and the `user` role is used to ask questions or provide input to the model.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to generate\n",
    "messages=[\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n",
    "  ]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": \"aws-neuron/mixtral-instruct-seqlen-4096-bs-4-optimum-0-0-23\", # placholder, needed\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    # \"stop\": [\"</s>\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = llm.predict({\"messages\" :messages, **parameters})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, we tested infernece now lets build a cool demo which support streaming responses. [Amazon SageMaker supports streaming responses](https://aws.amazon.com/de/blogs/machine-learning/elevating-the-generative-ai-experience-introducing-streaming-support-in-amazon-sagemaker-hosting/) from your model. We can use this to stream responses, we can leverage this to create a streaming gradio application with a better user experience.\n",
    "\n",
    "We created a sample application that you can use to test your model. You can find the code in [gradio-app.py](../demo/sagemaker_chat.py). The application will stream the responses from the model and display them in the UI. You can also use the application to test your model with your own inputs. With `share=True` you can share the application with others, since gradio with create a public link for you valid for 72 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add apps directory to path ../apps/\n",
    "import sys\n",
    "sys.path.append(\"../demo\") \n",
    "from llama3_chat import create_gradio_app\n",
    "\n",
    "# create gradio app\n",
    "create_gradio_app(\n",
    "    llm.endpoint_name,           # Sagemaker endpoint name\n",
    "    session=sess.boto_session,   # boto3 session used to send request \n",
    "    system_prompt=\"You are an helpful Assistant, called Mixtral. Knowing everyting about AWS.\",\n",
    "    concurrency_count=4,         # Number of concurrent requests\n",
    "    share=True,                  # Share app publicly\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gradio](./imgs/gradio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark Mixtral 8x7B with llmperf on AWS Inferentia2\n",
    "\n",
    "We successfully deployed Mixtral 8x7B to Amazon SageMaker and tested it. Now we want to benchmark the model to see how it performs. We will use a [llmperf](https://github.com/philschmid/llmperf) fork with support for `sagemaker`.\n",
    "\n",
    "First lets install the `llmperf` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/philschmid/llmperf.git\n",
    "!pip install -e llmperf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the benchmark with the following command. We are going to benchmark using `5` concurrent users and max `100` requests. The benchmark will measure `first-time-to-token`, `latency (ms/token)` and `throughput (tokens/s)` full details can be found in the `results` folder\n",
    "\n",
    "_🚨Important🚨: This benchmark was initiatied on a instance in us-east-1. Network communication thorugh the internet can have impact on the `first-time-to-token` metric. If you want to measure the `first-time-to-token` correctly, you need to run the benchmark on the same host or your production region._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell llmperf that we are using the messages api\n",
    "!MESSAGES_API=true python llmperf/token_benchmark_ray.py \\\n",
    "--model {llm.endpoint_name} \\\n",
    "--llm-api \"sagemaker\" \\\n",
    "--max-num-completed-requests 100 \\\n",
    "--timeout 600 \\\n",
    "--num-concurrent-requests 5 \\\n",
    "--results-dir \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets parse the results and display them nicely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "# Reads the summary.json file and prints the results\n",
    "with open(glob.glob(f'results/*summary.json')[0], 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(\"Concurrent requests: 50\")\n",
    "print(f\"Avg. Input token length: {int(data['results_number_input_tokens_mean'])}\")\n",
    "print(f\"Avg. Output token length: {int(data['results_number_output_tokens_mean'])}\")\n",
    "print(f\"Avg. Time-to-first-Token: {data['results_ttft_s_mean']*1000:.2f}ms\")\n",
    "print(f\"Avg. Inter-Token-Latency: {data['results_inter_token_latency_s_mean']*1000:.2f}ms/token\")\n",
    "print(f\"Avg. Thorughput: {data['results_mean_output_throughput_token_per_s']:.2f} tokens/sec\")\n",
    "print(f\"Failed Requests: {data['results_number_errors']}\")\n",
    "print(f\"Request per minute (RPM): {data['results_num_completed_requests_per_min']:.2f} req/min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the benchmark for different concurrent requests and got the following results:\n",
    "\n",
    "Mixtral 8x7B results on inf2.48xlarge:\n",
    "| Metric                          | 1             | 2              | 5              | 10            | 25             |\n",
    "|---------------------------------|---------------|----------------|----------------|---------------|----------------|\n",
    "| Avg. Input Token Length         | 568           | 559            | 562            | 538           | 561            |\n",
    "| Avg. Output Token Length        | 676           | 668            | 676            | 658           | 667            |\n",
    "| Avg. Time-to-First-Token (ms)   | 643.33        | 890.52         | 2435.47        | 5977.68       | 11051.98       |\n",
    "| Avg. Inter-Token Latency (ms/token) | 6.45       | 7.44           | 10.67          | 17.97         | 34.05          |\n",
    "| Avg. Throughput (tokens/sec)    | 136.06        | 193.89         | 288.00         | 337.28        | 354.68         |\n",
    "| Requests per Minute (RPM)       | 12.07         | 17.41          | 25.54          | 30.72         | 31.87          |\n",
    "\n",
    "\n",
    "\n",
    "We achieved a throughput of `288.00 tokens/sec` with an average inter-token latency of `10.67ms/token` and a time-to-first-token of `2435.47ms` for Mixtral 8x7B on inf2.48xlarge with 5 concurrent requests. The fastest latency was `6.45ms/token` with a time-to-first-token of `643.33ms` at 1 concurrent request. \n",
    "\n",
    "While scaling the number of concurrent requests, we observed that throughput peaked before reaching 10 concurrent users, as the thorughput and number of requests not increased afterwards. We would need to increase the number of replicas or batch size to improve the throughput. Scaling beyond 50 concurrent users, will lead to timeouts on the SageMaker side since requests are processed for longer than 60s. The inf2.48xlarge instance costs \$12.98/hour on-demand and \$7.79/hour with a 1-year savings plan for EC2.\n",
    "\n",
    "This benchmark is a good start to understand the performance of Mixtral 8x7B, but if you plan to use the model in production, we recommend to run a longer, more optimal detailed benchmark. Using your own data and moving client and host into the correct infrastructure setup. We successfully deployed, tested and benchmarked Mixtral 8x7B on AWS Inferentia2. 🎉 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
